{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e68a1f4-8baa-48ca-a63b-c5cc5652cfc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LLMs asking Questions on Private Data\n",
    "\n",
    "My notes on using LLMs to query private data. In particular using [LangChain](https://python.langchain.com/en/latest/index.html) to create embeddings, persist in a vector stores, and use with LLM.\n",
    "\n",
    "\n",
    "## LLMs and constraints\n",
    "\n",
    "LLMs are trained on large amounts of unstructured data and are great at general text generation. There are a few limitations of using off-the-shelf pre-trained LLMs:\n",
    "* They’re usually trained offline, making the model agnostic to the latest information\n",
    "* They make predictions by only looking up information stored in its parameters, leading to inferior interpretability.\n",
    "* They’re mostly trained on general domain corpora, making them less effective on domain-specific tasks. \n",
    "* There are scenarios when you want models to generate text based on specific data rather than generic data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e8ff0-d67f-48df-ba71-3652439a4075",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "\n",
    "<img src='./data/RAG2.png' width='1000'>\n",
    "\n",
    "RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. \n",
    "\n",
    "The idea of [Retrieval Augmented Generation (RAG)](https://huggingface.co/docs/transformers/model_doc/rag) workflow is simple. Instead of asking a question directly, the process first uses the user question to perform a search to retrieve relevant documents from the internal dataset and then provides these documents together with the question to LLM. With the additional context the LLM can answer as though it has been trained with the internal dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec67bb6-0023-49b0-aa10-b62f568b7105",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "\n",
    "[LangChain](https://python.langchain.com/en/latest/) is an opensource framework designed to simplify the creation of applications using LLMs. It includes a standard interface for interacting with LLMs. It allows chaining together different components to create more advanced use cases around LLMs. \n",
    "\n",
    "\n",
    "Key Modules:\n",
    "* **LLMs** OpenAI, Huggingface. Supported [integrations](https://langchain.com/integrations.html)\n",
    "* **Prompt templates**\n",
    "* **Agents** allow interactions tools like web search, external APIs, ...\n",
    "* **Memory** Short-term memory (chat history)/long-term memory (vector stores) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc3df4-3688-4a6c-bf30-b7955c9799b0",
   "metadata": {},
   "source": [
    "# Demo\n",
    "\n",
    "\n",
    "~/projects/llm\n",
    "\n",
    "```\n",
    "streamlit run app.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c0327-39e1-4ba2-9d26-f1a95c64f37e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! streamlit run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5621ed9-b7a1-4816-bf7b-cd1d5f42b092",
   "metadata": {},
   "source": [
    "-----\n",
    "## Environment set up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b5abc-e545-4b0d-b1e8-9f3e64083ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain --upgrade\n",
    "!pip install openai --upgrade\n",
    "!pip install pypdf --upgrade\n",
    "!pip install chromadb --upgrade\n",
    "!pip install pinecone-client --upgrade\n",
    "!pip install ipywidgets --upgrade\n",
    "!pip install tiktoken --upgrade\n",
    "!pip install langflow --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699368d-ddd4-40bb-92b6-73b126a14041",
   "metadata": {
    "hide_input": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check to see if there is an environment variable with your API keys, if not, use what you put below\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', 'YourAPIKey')\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', 'YourAPIKey')\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'us-west4-gcp') # You may need to switch with your env\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5427186-ac30-46df-a2a3-ff36e27d526c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---------\n",
    "# Data Ingestion\n",
    "\n",
    "\n",
    "<img src='./data/data_ingestion.png' width='800'>\n",
    "\n",
    "\n",
    "\n",
    "--> Point to data source and load multiple documents (PDF/Word/HTML/Chat...). [Document Loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)\n",
    "\n",
    "--> **Chunk** into smaller parts. [Text Splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)\n",
    "  * Optimize for the smallest size without losing context\n",
    "  * Consider adding some meaningful global metadata in all the chunks giving global context to all your embedded chunks\n",
    "  * Use ```chunk_overlap``` to maintain some local context\n",
    "  \n",
    "--> Create **embedding** vectors for each chunk using LLM embedding. [Text Embedding Models](https://python.langchain.com/en/latest/modules/models/text_embedding.html)\n",
    "  * An embedding is a vector (list) of floating point numbers\n",
    "        \n",
    "--> Raw data --> Embedding Model --> Vector Embedding --> Store embedding + meta data in \n",
    "        [Vector stores](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)\n",
    "\n",
    "  * **Vector stores**:\n",
    "    * [Pinecone](https://docs.pinecone.io/docs/overview): Managed vector store. Pinecone vector search index (Dimension: 1536)\n",
    "    * [Chroma](https://docs.trychroma.com/): Open source locally managed vector store.\n",
    "    \n",
    "--> **Semantic search** to retrieve relevant information by measuring the similarity between two vectors. Typically **Cosine, Dot Product, Euclidean** metric\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c07dd-f1df-44dd-8420-2969b34f9c43",
   "metadata": {},
   "source": [
    "-----\n",
    "## Load documents and chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6c0b6-cb96-4158-8db1-bb5816349913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting some variable used global for the following cells\n",
    "\n",
    "persist_chroma_directory = '.chroma_db'\n",
    "pdf_folder = './data/pdf'\n",
    "\n",
    "os.listdir(pdf_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a0f33-2b37-4923-b6f2-48cf94b13de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, \\\n",
    "                                        PyPDFLoader, \\\n",
    "                                        UnstructuredPDFLoader, \\\n",
    "                                        TextLoader\n",
    "\n",
    "loader = DirectoryLoader(pdf_folder, glob='**/*.pdf', loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# If using PyPDFLoader each document in documents is 1 page of a pdf. \n",
    "print(f'{len(documents)} pages loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87245c39-f766-4efc-b485-e831b5bb1b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f4a05-c6ce-4235-bc5c-0d08326831b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278110f-041e-41a6-845e-9c78a01ea7eb",
   "metadata": {},
   "source": [
    "----\n",
    "## Split in to smaller chunks\n",
    "\n",
    "* Split the text up into small, semantically meaningful chunks.\n",
    "* Most LLMs are constrained by the number of tokens that you can pass in so passing in an entire document or several document pages + prompt may exceed LLM token limit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84b93e-33dd-4851-bfcf-84653f62be2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Chunk loaded documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f'{len(chunks)} chunks created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d49ae-a824-4d7b-99c1-e1d26c13d401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks[:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c68da4-40de-483a-a39b-d133c65ee3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d1052-9776-42ee-a48d-4e417957dc54",
   "metadata": {},
   "source": [
    "----\n",
    "## Chroma: Create document embeddings\n",
    "\n",
    "[Chroma](https://docs.trychroma.com/): Open source locally managed vector store.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d14dfe-d15b-44be-92fd-16450abda218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "persist_chroma_directory = '.chroma_db'\n",
    "\n",
    "# use OpenAI embedding\n",
    "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, \\\n",
    "                             model='text-embedding-ada-002')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5b2b1-e3a0-4774-8af5-000b0462fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_store = Chroma.from_documents(documents=chunks, embedding=embedding, persist_directory=persist_chroma_directory)\n",
    "\n",
    "# Persist the database --> Need to call persist() when using Jupyter\n",
    "chroma_store.persist()\n",
    "chroma_store = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6631dd4a-818f-4e88-89f1-6f2fedc9b66f",
   "metadata": {},
   "source": [
    "### or Load previously stored embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b5f90-3775-437a-8cb2-b8600ae90604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "chroma_store = Chroma(embedding_function=embedding, persist_directory=persist_chroma_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbc8187-5bac-4e7f-a96a-ca39e7ccd723",
   "metadata": {},
   "source": [
    "### Similarity search in Chroma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f286a-5e11-492f-b138-f4730dec2a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the following to access the local Chroma store created by langchain instead of accessing Docker container\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "#persist_chroma_directory = '.chroma_db'\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(chroma_db_impl=\"duckdb+parquet\",\n",
    "                                    persist_directory=persist_chroma_directory\n",
    "                                    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ddb9b-243e-41e0-83fb-201106d695e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chroma_client.list_collections()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b26afe4-fe17-4f81-8181-c7a1a11c9609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "openai_embedding = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key = OPENAI_API_KEY,\n",
    "                model_name = \"text-embedding-ada-002\"\n",
    "                )\n",
    "\n",
    "collection = chroma_client.get_collection(name=\"langchain\", embedding_function=openai_embedding)\n",
    "\n",
    "collection.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b8a4e-3392-4f7c-b288-4fd3d1d8a4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Can I get hotel accomodation if my flight is cancelled?\"\n",
    "\n",
    "chunks = collection.query(query_texts=[query], n_results=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b4804-d410-4893-b4db-4f1740044a07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706aa083-8aa3-4645-a7bf-32897b9cd698",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----\n",
    "## Pinecone: Create document embeddings\n",
    "\n",
    "[Pinecone](https://app.pinecone.io/) Vectorstore is a fully managed vector database.\n",
    "\n",
    "\n",
    "\n",
    "Pinecone [CRUD](https://towardsdatascience.com/crud-with-pinecone-ee6b6f8b54e8) operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5300e4-7754-4d6e-8d41-f0aeb6c3c7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "\n",
    "\n",
    "index_name = \"langchaintest\" \n",
    "\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY, \n",
    "    environment=PINECONE_API_ENV\n",
    ")\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=1536    # OpenAI used 1536 dimension size\n",
    "    )\n",
    "\n",
    "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, \\\n",
    "                             model='text-embedding-ada-002')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b95e8-2455-4df2-8eea-2621f45a6bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create embedding and store in Pinecone\n",
    "\n",
    "pinecone_store = Pinecone.from_documents(documents=chunks, embedding=embedding, index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416fe0ce-e380-4f81-9391-40e01816af04",
   "metadata": {},
   "source": [
    "### or Load previously stored embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600fde31-db5b-4ced-8f77-f837f2aead15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively using previously created Pinecone index\n",
    "\n",
    "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "pinecone_store = Pinecone.from_existing_index(embedding=embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd6824-8dac-413b-bed9-baa31273f91f",
   "metadata": {},
   "source": [
    "### Similarity search in Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff671a11-349e-4d21-95a0-a2712ebcb6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Semantic search in Pinecone\n",
    "\n",
    "query = \"Can I get hotel accomodation if my flight is cancelled?\"\n",
    "chunks = pinecone_store.similarity_search(query, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa72be59-3122-4fbf-b07d-83c970572aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27c27d-75db-4e69-9f16-e18a1ca5e16f",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "# Query LLM\n",
    "\n",
    "\n",
    "<img src='./data/RAG.jpg' width='800'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4585981-a78c-454a-90d6-62b5048f7af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "\n",
    "# Can load the persisted database from disk, and use it as normal. \n",
    "if chroma_store == None:\n",
    "    chroma_store = Chroma(embedding_function=embedding, persist_directory=persist_chroma_directory)\n",
    "\n",
    "# Create the chain. Use ```retreiver``` to load previously generated embeddings from Chroma\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=chroma_store.as_retriever(search_kwargs={'k':3}), verbose=True)\n",
    "\n",
    "\n",
    "query = \"Provide details of compensation if my flight is cancelled? Output the results in bullet points\"\n",
    "#query = \"How much liquid can I bring on a flight?\"\n",
    "#query = \"how long is my ticket valid for?\"\n",
    "\n",
    "result = qa_with_sources(query)\n",
    "\n",
    "print(result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d226153-a0fc-4327-8d99-46f7df8d1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Question: {result[\"question\"]}\\n')\n",
    "\n",
    "print(f'Answer: {result[\"answer\"]}')\n",
    "\n",
    "for index, doc in enumerate(result['sources'].split(',')):\n",
    "    print(f'Source {index}: {doc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1e9be-a4d6-4e5c-9ac5-eb8701c6b95d",
   "metadata": {},
   "source": [
    "# BONUS: Low code LLM app builders\n",
    "\n",
    "\n",
    "\n",
    "* [FlowiseAI](https://flowiseai.com)\n",
    "\n",
    "* [LangFlow](https://github.com/logspace-ai/langflow) is a low code GUI builder for LLM application. Built on top of LangChain. Can be installed locally or find a hosted version on [Hugging Face](https://huggingface.co/spaces/Logspace/LangFlow)\n",
    "\n",
    "    ```\n",
    "    pip install langflow\n",
    "\n",
    "    python -m langflow\n",
    "\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730487f3-3548-487c-8144-7bafdb509875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m langflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf748de-9ba1-4b79-b996-d551e025e4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 mylangflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21776894-c679-497b-9afb-232bdc1adb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
