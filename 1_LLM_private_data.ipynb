{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e68a1f4-8baa-48ca-a63b-c5cc5652cfc8",
   "metadata": {},
   "source": [
    "# LLMs asking Questions on Private Data\n",
    "\n",
    "My notes on using LLMs to query private data. In particular using [LangChain](https://python.langchain.com/en/latest/index.html) to create embeddings, persist in a vector stores, and use with LLM.\n",
    "\n",
    "\n",
    "## LLMs and constraints\n",
    "\n",
    "LLMs are trained on large amounts of unstructured data and are great at general text generation. There are a few limitations of using off-the-shelf pre-trained LLMs:\n",
    "* They’re usually trained offline, making the model agnostic to the latest information\n",
    "* They make predictions by only looking up information stored in its parameters, leading to inferior interpretability.\n",
    "* They’re mostly trained on general domain corpora, making them less effective on domain-specific tasks. \n",
    "* There are scenarios when you want models to generate text based on specific data rather than generic data.\n",
    "\n",
    "\n",
    "## LangChain\n",
    "\n",
    "[LangChain](https://python.langchain.com/en/latest/) is an opensource framework designed to simplify the creation of applications using LLMs. It includes a standard interface for interacting with LLMs. It allows chaining together different components to create more advanced use cases around LLMs. \n",
    "\n",
    "\n",
    "Key Modules:\n",
    "* **Prompt templates**\n",
    "* **LLMs** OpenAI, Huggingface. Supported [integrations](https://langchain.com/integrations.html)\n",
    "* **Agents** allow interactions tools like web search, external APIs, ...\n",
    "* **Memory** Short-term memory (chat history)/long-term memory (vector stores) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc3df4-3688-4a6c-bf30-b7955c9799b0",
   "metadata": {},
   "source": [
    "# Demo\n",
    "\n",
    "\n",
    "~/projects/openai/langchain\n",
    "```\n",
    "streamlit run app.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c0327-39e1-4ba2-9d26-f1a95c64f37e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! streamlit run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5621ed9-b7a1-4816-bf7b-cd1d5f42b092",
   "metadata": {},
   "source": [
    "-----\n",
    "## Environment set up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b5abc-e545-4b0d-b1e8-9f3e64083ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain --upgrade\n",
    "!pip install openai --upgrade\n",
    "!pip install pypdf --upgrade\n",
    "!pip install chromadb --upgrade\n",
    "!pip install pinecone-client --upgrade\n",
    "!pip install ipywidgets --upgrade\n",
    "!pip install tiktoken --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699368d-ddd4-40bb-92b6-73b126a14041",
   "metadata": {
    "hide_input": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check to see if there is an environment variable with your API keys, if not, use what you put below\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', 'YourAPIKey')\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', 'YourAPIKey')\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'us-west4-gcp') # You may need to switch with your env\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5427186-ac30-46df-a2a3-ff36e27d526c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---------\n",
    "# Data Ingestion\n",
    "\n",
    "\n",
    "<img src='./data/data_ingestion.png' width='800'>\n",
    "\n",
    "\n",
    "\n",
    "--> Point to data source and load multiple documents (Pdf/Word/HTML/Chat...). [Document Loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)\n",
    "\n",
    "--> **Chunk** into smaller parts. [Text Splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)\n",
    "  * Optimize for the smallest size without losing context\n",
    "  * Consider adding some meaningful global metadata in all the chunks giving global context to all your embedded chunks\n",
    "  * Use ```chunk_overlap``` to maintain some local context\n",
    "  \n",
    "--> Create **embedding** vectors for each chunk using LLM embedding. [Text Embedding Models](https://python.langchain.com/en/latest/modules/models/text_embedding.html)\n",
    "  * An embedding is a vector (list) of floating point numbers\n",
    "        \n",
    "--> Raw data --> Embedding Model --> Vector Embedding --> Store embedding + meta data in \n",
    "        [Vectorstores](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)\n",
    "\n",
    "  * **Vector stores**:\n",
    "    * [Pinecone](https://docs.pinecone.io/docs/overview): Managed vector store. Pinecone vector search index (Dimension: 1536, Metric: [Cosine, DotProduct, Euclidean])\n",
    "    * [Chroma](https://docs.trychroma.com/): Open source locally managed vector store.\n",
    "    \n",
    "--> **Semantic search** to retrieve relevant informaiton by measuring the distance between two vectors ie. measures their similarity\n",
    "  \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6c0b6-cb96-4158-8db1-bb5816349913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting some variable used global for the following cells\n",
    "\n",
    "persist_chroma_directory = '.chroma_db'\n",
    "pdf_folder = './data/pdf'\n",
    "\n",
    "os.listdir(pdf_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a0f33-2b37-4923-b6f2-48cf94b13de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, \\\n",
    "                                        PyPDFLoader, \\\n",
    "                                        UnstructuredPDFLoader, \\\n",
    "                                        TextLoader\n",
    "\n",
    "loader = DirectoryLoader(pdf_folder, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# If using PyPDFLoader each document in documents is 1 page of a pdf. \n",
    "print(f'{len(documents)} pages loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87245c39-f766-4efc-b485-e831b5bb1b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f4a05-c6ce-4235-bc5c-0d08326831b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d1052-9776-42ee-a48d-4e417957dc54",
   "metadata": {},
   "source": [
    "## Create document embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d14dfe-d15b-44be-92fd-16450abda218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# use OpenAI embedding\n",
    "embedding = OpenAIEmbeddings()\n",
    "persist_chroma_directory = '.chroma_db'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5b2b1-e3a0-4774-8af5-000b0462fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_store = Chroma.from_documents(documents=chunks, embedding=embedding, persist_directory=persist_chroma_directory)\n",
    "\n",
    "# Persist the database --> Need to call persist() when using Jupyter\n",
    "chroma_store.persist()\n",
    "chroma_store = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6631dd4a-818f-4e88-89f1-6f2fedc9b66f",
   "metadata": {},
   "source": [
    "## or Load previously stored embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b5f90-3775-437a-8cb2-b8600ae90604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "chroma_store = Chroma(embedding_function=embedding, persist_directory=persist_chroma_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27c27d-75db-4e69-9f16-e18a1ca5e16f",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "# Query LLM\n",
    "\n",
    "\n",
    "<img src='./data/RAG.jpg' width='800'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4585981-a78c-454a-90d6-62b5048f7af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Can load the persisted database from disk, and use it as normal. \n",
    "if chroma_store == None:\n",
    "    chroma_store = Chroma(embedding_function=embedding, persist_directory=persist_chroma_directory)\n",
    "\n",
    "# Create the chain\n",
    "#   Use ```retreiver``` in future calls to load previously generated embeddings from Chroma\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=chroma_store.as_retriever(search_kwargs={'k':3}), verbose=True)\n",
    "\n",
    "\n",
    "query = \"Provide details of my hotel entitlement if my flight is cancelled?\"\n",
    "#query = \"How much liquid can I bring on a flight?\"\n",
    "#query = \"how long is my ticket valid for?\"\n",
    "result = qa.run(query)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d226153-a0fc-4327-8d99-46f7df8d1137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1417722-531c-450e-b512-f83937942e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
