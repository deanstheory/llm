{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a979bba-523b-4897-b8ad-9a5e33292978",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "----------------------\n",
    "# Open vs Closed Models\n",
    "\n",
    "\n",
    "![Open vs Closed](./data/open-v-closed.jpg)\n",
    "\n",
    "\n",
    "Proprietary models are still the top performing general purpose LLM. These models are trained on vast amounts of data at considerable expense. Some companies have released smaller versions of their models under different open frameworks and licenses\n",
    "\n",
    "\n",
    "## Closed/Proprietary LLMs\n",
    "\n",
    "Closed-source LLMs, keep their internal workings under wraps. These models are typically created by well-funded companies that can pour resources into development and ongoing refinement.\n",
    "\n",
    "* Limited Access: You might need special permission or a paid subscription to use them.\n",
    "* Customisation: Fine-tuning pre-trained models and adjusting parameters may be your only customization options, as the underlying code and architecture are often inaccessible.\n",
    "* Support: The companies that develop these models often provide vendor support, which can be helpful if you lack in-house expertise.\n",
    "* Proprietary Licensing: Expect specific terms and conditions for using the LLM, as the ownership remains with the developer.\n",
    "\n",
    "eg: OpenAI ChatGPT, Google Gemini, Antropic Claude, Cohere Command\n",
    "\n",
    "\n",
    "## Open LLM\n",
    "\n",
    "Most of the top performing open models are derived from closed models or much larger models. Open models are freely available to the public. This fosters innovation, collaboration, and community driven development\n",
    "\n",
    "* Customization: Open access to the model's inner workings allows for deep customization, innovation, tailoring the LLM to specific needs.\n",
    "  * Quantisation\n",
    "  * LoRA\n",
    "* Community-Driven Support: A large and active developer community provides assistance, fostering collaboration and knowledge sharing.\n",
    "* Licensing: Depending on the chosen open-source license, commercial or research use might require specific actions.\n",
    "* Transparency builds trust: Open development helps fostering trust and reliability.\n",
    "* Accessible base models (LLaMA, Mistral): Several high-quality, open base models are readily available, ranging from smaller (3B-13B parameters) to larger options (30B-70B parameters). While these may not be the absolute biggest models, they offer a good balance of power and accessibility.\n",
    "* Open models going thru' very quick iterations (every 1~2 wks)\n",
    "\n",
    "eg. LLaMA family, Mistral, DBRX\n",
    "\n",
    "\n",
    "\n",
    "### Open source vs open weight\n",
    "\n",
    "This is a critical question in the field of large language models (LLMs). While releasing a model's weights can enable others to use the model for certain tasks, it doesn't provide full transparency or allow for retraining the model from scratch. Here's a breakdown of the two approaches:\n",
    "\n",
    "**Open Source:**\n",
    "\n",
    "True open-source release involves sharing everything necessary to rebuild and train the model from the ground up. This includes:\n",
    "* Model architecture code: The blueprint for the LLM's structure and organization.\n",
    "* Training methodology and hyperparameters: The specific steps and settings used to train the model.\n",
    "* Original training dataset: The data the model was trained on, which can significantly impact its performance and biases.\n",
    "* Documentation and other relevant details: Any additional information to aid in understanding and using the model.\n",
    "\n",
    "Open-sourcing a LLM offers several advantages:\n",
    "* Transparency: Anyone can inspect the inner workings of the model, fostering trust and reliability.\n",
    "* Customization: Users can tailor the model to their specific needs by modifying the code and training data.\n",
    "* Collaboration: An open-source LLM fosters a community of developers who can contribute to its improvement.\n",
    "\n",
    "However, releasing a LLM as open-source requires a substantial commitment from the developers, as it necessitates making proprietary information public.\n",
    "\n",
    "\n",
    "**Open Weights:**\n",
    "\n",
    "Releasing only the model's weights is a less comprehensive approach to open-sourcing. The weights are the learned parameters of the neural network that determine the model's behavior. While releasing the weights allows others to:\n",
    "* Use the model for inference: Apply the model to new data to generate predictions.\n",
    "* Fine-tune the model: Adjust the model's behavior for a specific task by training it on additional data.\n",
    "\n",
    "Crucially, it doesn't allow for:\n",
    "* Retraining the model from scratch: Users cannot rebuild the model with different data or training settings.\n",
    "* Understanding how the model works: The inner workings of the model remain opaque without access to the training code and dataset.\n",
    "\n",
    "\n",
    "\n",
    "![Open vs close model ELO](./data/arena_elo.jpg)\n",
    "\n",
    "\n",
    "\n",
    "-----------------\n",
    "References:\n",
    "\n",
    "* Leaked Google document: [We Have No Moat, And Neither Does OpenAI](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)\n",
    "* https://promptengineering.org/llm-open-source-vs-open-weights-vs-restricted-weights/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c3452-b321-4a84-8943-20209e5844a5",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# Popular open models\n",
    "\n",
    "\n",
    "## LLaMa Family\n",
    "\n",
    "* LLaMA (Large Language Model Meta AI) are a family of LLM models release by [Meta](https://ai.meta.com/blog/meta-llama-3/). Original model released to researchers under non-commercial license (Mar '23) however model weights were leaked.\n",
    "* A large number of researchers have extended LLaMA models by either instruction tuning or continual pretraining\n",
    "  *  Instruction tuning LLaMA has become a major approach to developing customized or specialized models, due to the relatively low computational costs.\n",
    "* LLaMA is available on HuggingFace [Meta profile](https://huggingface.co/meta-llama) a \n",
    "* [Purple Llama](https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai) project covering tools and evaluations to help developers build responsibly with open generative AI models\n",
    "* LLaMa 3 release Apr '24\n",
    "\n",
    "\n",
    "\n",
    "![LLaMA](./data/llama.png)\n",
    "\n",
    "\n",
    "* Stanford researcher created the [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) by fine-tuning the LLaMA-7B model using the [Alpaca-52K](https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json) instruction-following data \n",
    "*  Alpaca outperforms the LLaMA model and only cost about \\$500 to train.\n",
    "* [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) fine-tuned LLaMa model using 70K user-shared ChatGPT conversations. The cost of training Vicuna-13B was around \\$300.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------------------\n",
    "## Mistral\n",
    "\n",
    "![Mistral](./data/mistral.png)\n",
    "\n",
    "\n",
    "* Mistral provides 2 open weights models that can be used without restrictions (Apache 2.0 license)\n",
    "  * Mistral-7B\n",
    "  * Mixtral-8x22B mixtral of experts (MoE) model\n",
    "* Mistral don't expose how models were trained, recipe, how to collect the data\n",
    "* Multiple language support in models\n",
    "* Focus on developer experience\n",
    "* Weights available on HuggingFace [Mistral AI profile](https://huggingface.co/mistralai)\n",
    "\n",
    "\n",
    "-----------------------\n",
    "## DBRX\n",
    "\n",
    "* DBRX is a general purpose LLM created by [Databricks](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)\n",
    "* Mixture of experts (MoE) architecture --> a large number of smaller experts. DBRX has 16 experts and chooses 4\n",
    "* Weights available on HuggingFace [Databrick profile](https://huggingface.co/databricks)\n",
    "\n",
    "\n",
    "-----------------------\n",
    "## Other open models\n",
    "\n",
    "\n",
    "### Google\n",
    "\n",
    "* [Gemma](https://ai.google.dev/gemma) is a family of lightweight open models from Google\n",
    "\n",
    "\n",
    "### Microsoft\n",
    "\n",
    "* Microsoft [Phi 3](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) family of small curated data models\n",
    "\n",
    "\n",
    "### Apple\n",
    "\n",
    "* Apple [OpenELM](https://huggingface.co/apple/OpenELM) family of models released April '24\n",
    "* These are small language models with a focus of embedding in devices\n",
    "* Pay attention to the license\n",
    "\n",
    "\n",
    "### Snowflake\n",
    "\n",
    "* Snowflake released a massive 480B parameter open model called [Arctic](https://huggingface.co/Snowflake/snowflake-arctic-instruct) in April '24\n",
    "* Release under Apache 2.0 license\n",
    "\n",
    "  \n",
    "### Grok (X)\n",
    "\n",
    "* Open weights\n",
    "* Available on HuggingFace [xAI profile](https://huggingface.co/xai-org)\n",
    "\n",
    "\n",
    "### Alibaba \n",
    "\n",
    "* [Qwen](https://github.com/QwenLM/Qwen) is a family of models from Alibaba\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a47937-590d-4850-b643-6aa554549776",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "-----------------------\n",
    "# Running LLMs locally\n",
    "\n",
    "  \n",
    "## Ollama\n",
    "\n",
    "* [Ollama](https://ollama.com/) \n",
    "  * Simple and fast\n",
    "  * Large range of [models](https://ollama.com/library) available\n",
    "  * ```ollama pull llama3```\n",
    "  * ```ollama run llama3```\n",
    "  * [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)\n",
    "    * ```ollama create my-model-name -f my-model-file```\n",
    "    * ```ollma run my-model-name```\n",
    "     \n",
    "\n",
    "-----------------------\n",
    "## GTP4ALL\n",
    "\n",
    "* [GPT4ALL](https://gpt4all.io/)\n",
    "  * Can add local data collection enabling chat with you private data\n",
    "  * Limited models\n",
    "   \n",
    "\n",
    "-----------------------\n",
    "## LLaMa.cpp\n",
    "\n",
    "* [LLaMa.cpp](https://github.com/ggerganov/llama.cpp)\n",
    "  * Open source implementation of Meta's LLaMa architecture in C/C++\n",
    "  * Minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud\n",
    "  * Used as the inference engine in tools lie Ollama\n",
    "  * Uses **GGUF** format. Modern and efficient model format that bundles all necessary information for loading and running a model\n",
    "  * Supports most popular open weight models including Llama, Mistral, Gemma\n",
    "   \n",
    "\n",
    "----------------------\n",
    "## Transformers.js - Run transformers in the browser\n",
    "\n",
    "Hugging Face [Transformers.js](https://huggingface.co/docs/transformers.js/en/index) allows you to run ML/transformers directly in your browser using Microsoft [ONNX](https://onnxruntime.ai/) runtime\n",
    "* No server compute\n",
    "* Privacy - no data sent to servers\n",
    "* Rich JS tools to interact with browser\n",
    "* [WebGPU](https://caniuse.com/webgpu) support --> speed\n",
    "\n",
    "\n",
    "---------------------------\n",
    "## Other alternatives - Groq\n",
    "\n",
    "* Specialised chip to run GenAI inference [Groq cloud](https://console.groq.com/playground)\n",
    "* Groq positions itself as a challenger to Nvidia with its Language Processing Unit (LPU) chips as demand for specialized computer chips has skyrocketed\n",
    "* https://simonwillison.net/2024/Apr/22/llama-3/\n",
    "\n",
    "\n",
    "\n",
    "---------------------\n",
    "## Hugging Face\n",
    "\n",
    "* [Hugging Face](https://huggingface.co/) Github of machine learning\n",
    "  * Models\n",
    "  * **Datasets** eg. (Banking77](https://huggingface.co/datasets/PolyAI/banking77)\n",
    "  * Spaces: Build and publish ML applications\n",
    "* If interested in building apps using Hugging Face models learn the [Transformers Library](https://huggingface.co/docs/transformers/index)\n",
    "  *  Open-source implementations of transformer models for text, image, and audio tasks\n",
    "  *  [Transformers.js](https://huggingface.co/docs/transformers.js/index) run transformers/ML directly in your browser eliminating the need for a server.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f8da9-2c2a-41c7-817c-ab296fc0af1c",
   "metadata": {},
   "source": [
    "------------------------\n",
    "# LLM Leaderboard\n",
    "\n",
    "* HuggingFace [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "* LMSYS Chatbot Arena [leaderboard](https://chat.lmsys.org/?leaderboard)\n",
    "\n",
    "\n",
    "\n",
    "## Model Benchmarks\n",
    "\n",
    "* [AI2 reasoning challenge](https://allenai.org/data/arc) (ARC) - grade school science questions\n",
    "* [HellaSwag](https://rowanzellers.com/hellaswag/) - common sense\n",
    "* [MMLU](https://github.com/hendrycks/test) (Massive Multitask Manguage Understanding) - how diverse LLM knowledge is\n",
    "* [TruthfulQA](https://github.com/sylinrl/TruthfulQA) - how truthful is a model\n",
    "* [WinoGrande](https://winogrande.allenai.org/) - commonsense reasoning\n",
    "* [GSM8K](https://paperswithcode.com/dataset/gsm8k) - maths reasoning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82648e2a-1fda-49b8-9df9-97438619a205",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# References\n",
    "\n",
    "\n",
    "## Survey Papers - arxiv.org\n",
    "* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "* [Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196.pdf)\n",
    "\n",
    "\n",
    "## HuggingFace\n",
    "\n",
    "* Hugging Face [EPAM profile](https://huggingface.co/epam)\n",
    "* HuggingFace [course](https://www.youtube.com/watch?v=00GKzGyWFEs&list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o) - Technical\n",
    "\n",
    "\n",
    "## LangChain\n",
    "\n",
    "* Langchain support for [open models](https://python.langchain.com/docs/integrations/llms/)\n",
    "\n",
    "\n",
    "## Learnings\n",
    "\n",
    "* DeepLearning.AI [short courses](https://www.deeplearning.ai/short-courses/)\n",
    "* [Understanding Deeplearning](https://udlbook.github.io/udlbook/) \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64925b52-c2f6-4e21-a988-36840abc8bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
