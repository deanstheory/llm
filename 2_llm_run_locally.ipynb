{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701766ba-8181-4632-b7ff-d1008334a6a5",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# Closed vs Open Models\n",
    "\n",
    "\n",
    "Proprietary model trained on vast amounts of data at considerable expense. Some companies have released smaller version of these models under different open frameworks and licenses\n",
    "\n",
    "\n",
    "## Closed/Proprietary LLMs\n",
    "\n",
    "OpenAI’s ChatGPT or Anthropic’s Claude are examples of proprietary systems where the public can’t access the code and model weights\n",
    "\n",
    "Closed source LLMs are language models where the model source or weights are not publicly available. Often developed by companies with significant resource for development and improvement. \n",
    "* Access may be restricted or a paid subsciption\n",
    "* Customisation may be limited as access to underlaying code and architecture mat be limit. Limiting customization to fine-tuning on pre-trained models and parmeter configuration\n",
    "* Vendor Support\n",
    "* Proprietary licensing\n",
    "* Advanced Features and Performance. Proprietary models usually maximum performance\n",
    "\n",
    "eg: OpenAI ChatGPT, Google Gemini, Antropic Claude, Cohere Command\n",
    "\n",
    "\n",
    "## Open LLM\n",
    "\n",
    "Most of the top performing open models are derived from closed models or much large models\n",
    "\n",
    "\n",
    "* Leaked LLama wieghts - Mar '23\n",
    "* Leaked Google document: [We Have No Moat, And Neither Does OpenAI](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)\n",
    " * Open models going thru' very quick iterations (every 1~2 wks)\n",
    "\n",
    "\n",
    "\n",
    "Open models are freely available to the public. This fosters innovation, collaboration, and community driven development\n",
    "* Customizability\n",
    "* Support is via the community. This can be large community of developers depending on the model\n",
    "* Licensing needs to be reviewed if commercial use or research use\n",
    "* Transparent development which can help build trust\n",
    "* Availability of high-quality open base models (LLaMA, Mistral \n",
    "* Fewer parameters: 3B, 13B, 30B, 70B\n",
    "  \n",
    "eg. LLaMA family, Mistral, DBRX\n",
    "\n",
    "\n",
    "### Open source vs open weight\n",
    "\n",
    "The core question is whether simply releasing a model’s weights while keeping training methodology and data proprietary can be considered true open sourcing. \n",
    "\n",
    "\n",
    "\n",
    "Releasing only a model's weights broadly enables application development but concentrates control among a small group of organizations. Enabling open source access distributes control but requires greater commitment to transparency and decentralization.\n",
    "\n",
    "* Open source\n",
    "  * releasing a model as open source would entail providing the full source code and information required for retraining the model from scratch. This includes the model architecture code, training methodology and hyperparameters, the original training dataset, documentation, and other relevant details.\n",
    "  * open source enables model understanding and customization but requires substantially more work to release.\n",
    "  * [Olmo](https://allenai.org/olmo)\n",
    "\n",
    "\n",
    "* Open weights isn’t open source unless they provide full access to their training set and source code.\n",
    "  * Open weights refers to releasing only the pretrained parameters or weights of the neural network model itself.\n",
    "  * This allows others to use the model for inference and fine-tuning.\n",
    "  * However, the training code, original dataset, model architecture details, and training methodology is not provided.\n",
    "  * open weights allows model use but not full transparency\n",
    "\n",
    "\n",
    "\n",
    "![Open vs close model ELO](./data/arena_elo.jpg)\n",
    "\n",
    "\n",
    "\n",
    "-----------------\n",
    "References:\n",
    "\n",
    "* https://www.linkedin.com/pulse/deep-dive-opensource-llms-vs-proprietor-dr-rabi-prasad-kutuc/\n",
    "* https://promptengineering.org/llm-open-source-vs-open-weights-vs-restricted-weights/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c3452-b321-4a84-8943-20209e5844a5",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# Popular open models\n",
    "\n",
    "\n",
    "## LLaMa Family\n",
    "\n",
    "* LLaMA (Large Language Model Meta AI) are a family of LLM models release by [Meta](https://ai.meta.com/blog/meta-llama-3/). Originally model released to researchers under non-commercial license (Mar '23) however model weights were leaked.\n",
    "* A large number of researchers have extended LLaMA models by either instruction tuning or continual pretraining\n",
    "  *  instruction tuning LLaMA has become a major approach to developing customized or specialized models, due to the relatively low computational costs.\n",
    "* LLaMA is available on HuggingFace [Meta profile](https://huggingface.co/meta-llama)\n",
    "\n",
    "\n",
    "![LLaMA](./data/llama.png)\n",
    "\n",
    "\n",
    "\n",
    "*  Stanford [Alpaca-52K](https://github.com/tatsu-lab/stanford_alpaca) instruction-following data generated by the techniques in the [Self-Instruct](https://github.com/yizhongw/self-instruct)\n",
    "*  On the self-instruct evaluation set, Alpaca shows many behaviors similar to OpenAI’s text-davinci-003, but is also surprisingly small and easy/cheap to reproduce.\n",
    "*  Alpaca: [fine-tune](https://github.com/tatsu-lab/stanford_alpaca?tab=readme-ov-file#fine-tuning)\n",
    "*  LLaMMA models using standard Hugging Face training code\n",
    "*  Alpaca cost about \\$500 to train.\n",
    "* [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)\n",
    "  * The cost of training Vicuna-13B is around \\$300.\n",
    "  * Non-commercial use\n",
    "\n",
    "\n",
    "\n",
    "---------------------\n",
    "## Mistral\n",
    "\n",
    "* Open weights NOT open source\n",
    "  * Open weights isn’t open source unless they provide full access to their training set and source code. In all respect to the capabilities of Mistral’s models, it is an extreme stretch to call company that’s dropping torrents of weight binary, an OPEN SOURCE\n",
    "  * Don't expose how training, recipe, how to collect the data, mixture of experts\n",
    "  * Currently focused on developer experience first\n",
    "  * Not just APIs --> because you need AI integrator\n",
    "* Weights available on HuggingFace [Mistral AI profile](https://huggingface.co/mistralai)\n",
    "\n",
    "\n",
    "\n",
    "----------------------\n",
    "## Grok (X)\n",
    "\n",
    "* Open weights\n",
    "* Available on HuggingFace [xAI profile](https://huggingface.co/xai-org)\n",
    "\n",
    "\n",
    "\n",
    "-----------------------\n",
    "## DBRX\n",
    "\n",
    "* DBRX is a general purpose LLM created by [Databricks](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)\n",
    "* Mixture of experts (MoE) architecture\n",
    "  * Routrer\n",
    "* DBRX is fine-grained, meaning it uses a larger number of smaller experts. DBRX has 16 experts and chooses 4, \n",
    "* Weights available on HuggingFace [Databrick profile](https://huggingface.co/databricks)\n",
    "\n",
    "\n",
    "-----------------------\n",
    "## References:\n",
    "\n",
    "*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a47937-590d-4850-b643-6aa554549776",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# Running LLMs locally\n",
    "\n",
    "\n",
    "* Laptop\n",
    "* No need to use external APIs\n",
    "* Own cloud with GPU/\n",
    "* * Groq\n",
    "* Run on phones\n",
    "\n",
    "  \n",
    "## Ollama\n",
    "\n",
    "[Analyse expenses with local LLM](https://www.youtube.com/watch?v=h_GTxRFYETY)\n",
    "\n",
    "\n",
    "  * [Ollama](https://ollama.com/) <--\n",
    "    * Simple and fast\n",
    "    * Large range of [models](https://ollama.com/library) available\n",
    "    * ```ollama pull mistral```\n",
    "    * ```ollama run mistral```\n",
    "    * [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)\n",
    "      * ```ollama create my-model-name -f my-model-file```\n",
    "      * ```ollma run my-model-name```\n",
    "     \n",
    "\n",
    "-----------------------\n",
    "## GTP4ALL\n",
    "\n",
    "  * [GPT4ALL](https://gpt4all.io/) <--\n",
    "    * Chat with you private data. Can add own docs for context\n",
    "    * Plug-in \n",
    "    * Limited models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-----------------------\n",
    "## Hugging Face\n",
    "\n",
    "  * HuggingFace [Transformers](https://huggingface.co/docs/transformers/en/index)\n",
    "    * Need to know how to code (Python)\n",
    "    * ML knowledge\n",
    "   \n",
    "\n",
    "\n",
    "-----------------------\n",
    "## LLaMa.cpp\n",
    "\n",
    "\n",
    "\n",
    "  * [LLaMa.cpp](https://github.com/ggerganov/llama.cpp) <--\n",
    "    * Fast\n",
    "    * Csan run bigger models on smaller hardware\n",
    "    * Uses **GGUF** format. Modern and efficient\n",
    "    * Limited model support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2373172-daa4-4f1c-8081-29db30a6c62b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Running LLMs locally\n",
    "\n",
    "      \n",
    "Way??\n",
    "* Privacy\n",
    "* Compliance\n",
    "* Trust\n",
    "* Cost????\n",
    "* Smaller \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Locally\n",
    "\n",
    "\n",
    "* 6 ways:\n",
    "  * [Ollama](https://ollama.com/) <--\n",
    "    * Simple and fast\n",
    "    * Large range of [models](https://ollama.com/library) available\n",
    "    * ```ollama pull mistral```\n",
    "    * ```ollama run mistral```\n",
    "    * [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)\n",
    "      * ```ollama create my-model-name -f my-model-file```\n",
    "      * ```ollma run my-model-name```\n",
    "  * [GPT4ALL](https://gpt4all.io/) <--\n",
    "    * Chat with you private data. Can add own docs for context\n",
    "    * Plug-in \n",
    "    * Limited models\n",
    "  * HuggingFace [Transformers](https://huggingface.co/docs/transformers/en/index)\n",
    "    * Need to know how to code (Python)\n",
    "    * ML knowledge\n",
    "  * [LLaMa.cpp](https://github.com/ggerganov/llama.cpp) <--\n",
    "    * Fast\n",
    "    * Csan run bigger models on smaller hardware\n",
    "    * Uses **GGUF** format. Modern and efficient\n",
    "    * Limited model support\n",
    "  \n",
    "  * [LangChain](https://python.langchain.com/docs/integrations/platforms/)\n",
    "    * Run local and remote models\n",
    "    * Slow (Python)\n",
    "\n",
    "  * [llamafile](https://github.com/Mozilla-Ocho/llamafile)\n",
    "    * Embed model in executionable file and run anywhere\n",
    "    * Builds on llama.cpp\n",
    "    * Spped optimisation for various hardware - https://justine.lol/matmul/\n",
    "\n",
    "\n",
    "\n",
    "### Quantisation\n",
    "\n",
    "Reduce memory footprint for the model weights\n",
    "\n",
    "\n",
    "### File formats\n",
    "\n",
    "* Model and fine-tuning file formats\n",
    "  * Uses **GGUF** format. Modern and efficient\n",
    "\n",
    "\n",
    "\n",
    "## Popular LLM Families\n",
    "\n",
    "* OpenAI GPT family\n",
    "* Meta LLaMA family\n",
    "* Google PaLM family\n",
    "\n",
    "\n",
    "## Other alternatives - Groq\n",
    "\n",
    "* [Groq cloud](https://console.groq.com/playground)\n",
    "  * https://simonwillison.net/2024/Apr/22/llama-3/\n",
    "  * Serves at very high speeds - 800 tokens/sec\n",
    "  * Groq API (key required)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------------------------\n",
    "## References:\n",
    "\n",
    "* https://semaphoreci.com/blog/local-llm\n",
    "* Youtube [6 Ways to Run ChatGPT Alternatives in Your Machine](https://www.youtube.com/watch?v=7jMIsmwocpM)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39add882-f717-4369-80d8-c2350c765761",
   "metadata": {},
   "source": [
    "# Open Model Licensing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f8da9-2c2a-41c7-817c-ab296fc0af1c",
   "metadata": {},
   "source": [
    "# LLM Leaderboard\n",
    "\n",
    "* HuggingFace [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "* LMSYS Chatbot Arena [leaderboard](https://chat.lmsys.org/?leaderboard)\n",
    "\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a122f-7ed7-46b3-b33d-d1a45b4c464f",
   "metadata": {},
   "source": [
    "# Ollama & Private Data\n",
    "\n",
    "[Ollama](https://ollama.com/) download and install on Mac/Linux/Windows\n",
    "\n",
    "Available Ollama [models](https://ollama.com/library)\n",
    "* [Mistral](https://ollama.com/library/mistral)\n",
    "* Mixtral-8x7B is more powerful and can handle extended conversations - Supported????\n",
    "\n",
    "Ollama has support for multi-modal LLMs, such as bakllava and llava.\n",
    "\n",
    "\n",
    "Ollama & [Langchain](https://python.langchain.com/docs/integrations/llms/ollama)\n",
    "\n",
    "\n",
    "Basic commands:\n",
    "\n",
    "* Fetch available LLM model via  ```ollama pull <name-of-model>```\n",
    "* On Mac, the models will be download to ```~/.ollama/models```\n",
    "* To view all pulled models, use ```ollama list```\n",
    "* To chat directly with a model from the command line, use ```ollama run <name-of-model>```\n",
    "\n",
    "\n",
    "\n",
    "--------------------\n",
    "## References:\n",
    "\n",
    "* Building a Multi-Document Chatbot Using [Mistral 7B, ChromaDB, and Langchain](https://www.e2enetworks.com/blog/building-a-multi-document-chatbot-using-mistral-7b-chromadb-and-langchain)\n",
    "* [Ask Your Web Pages Using Mistral-7b & LangChain](https://medium.com/@zekaouinoureddine/ask-your-web-pages-using-mistral-7b-langchain-f976e1e151ca)\n",
    "* [Ollama Python Library Released! How to implement Ollama RAG?](https://www.youtube.com/watch?v=4HfSfFvLn9Q)\n",
    "  * Ollama RAG [code](https://mer.vin/2024/01/ollama-rag/)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec921a3-2683-4dcd-a1a7-8c325c6e954c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Why don't scientists trust atoms?\\n\\nBecause they make up everything!\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from langchain_community.llms import Ollama\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e71e1-b736-4e47-9c28-1a1893b7d9ba",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba1322-3b54-495e-b256-13dd40643992",
   "metadata": {},
   "source": [
    "-----\n",
    "## Environment set up\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18f5a5-15e0-4d88-8faa-99371aca6c11",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# Mistral Embedding\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e45c7b5-64a2-4d45-8cd5-e92e4b6602cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---------------------------\n",
    "# Other Stuff\n",
    "\n",
    "## Model Fine-tuning\n",
    "\n",
    "* LoRA (Low Rank)\n",
    "  * reduces the size of the update matrices by a factor of up to several thousand.\n",
    "  * This allows model fine-tuning at a fraction of the cost (~$100) and time (<1 day).\n",
    "  * Being able to personalize a language model in a few hours on consumer hardware is a big deal\n",
    "  * Fine-tuning is stackable\n",
    "  * Trianing giant models from scratch expensive and time consuming\n",
    "* Data quality scales better than data size\n",
    "* [ORPO](https://huggingface.co/blog/mlabonne/orpo-llama-3) finetuning <--\n",
    "  * Instruction tuning and preference alignment\n",
    " \n",
    "## Instruction tuning vs fine-tuning???\n",
    "\n",
    "\n",
    "## Training\n",
    "\n",
    "* Pre-training --> Fine-tuning --> Alignment\n",
    "* Fine-tuning (--> $100s cost???)\n",
    "  * An important reason to fine-tune LLMs is to align the responses to the expectations humans will have when providing instructions through prompts. This is the so-called instruction tuning\n",
    "  * AI Alignment is the process of steering AI systems towards human goals, preferences, and principles.\n",
    "    * Does each new application or idea really needs a whole new model?? or just fine-tune\n",
    "    * Align with human preference using DPO\n",
    "* Current open source fine-tuning the LLM in three ways:\n",
    "  * Reinforcement learning with human feedback (RLHF)\n",
    "  * Supervised Fine-tuning (SFT)\n",
    "  * Direct Preference Optimization (DPO). 3 main steps:\n",
    "    * Supervised fine-tuned model\n",
    "      * The process of taking a pre-trained model and further training it on a new, labeled dataset, with the goal of adapting the model's parameters to perform a specific task more effectively.\n",
    "    * The process of annotating data with preference labels\n",
    "    * DPO training\n",
    "  * https://cameronrwolfe.substack.com/p/understanding-and-using-supervised <---\n",
    "  * https://medium.com/@anchen.li/fine-tune-llama-2-with-sft-and-dpo-8b57cf3ec69\n",
    "* Transformer Reinforcement Learning [TRL](https://huggingface.co/docs/trl/index)\n",
    "\n",
    "\n",
    "\n",
    "![How to build a LLM](./data/build_llm.png)\n",
    "\n",
    "\n",
    "### Synthetic data\n",
    "\n",
    "*[Distilabel](https://github.com/argilla-io/distilabel)\n",
    "  * Framework for synthetic data and AI feedback for AI engineers\n",
    "  * AI generated AI datasets\n",
    "  * distilabel: Preference task\n",
    "    * Supervised fine tuning\n",
    "    * RLHF and DPO\n",
    "  * distilabel: Self-instruct task\n",
    "    *  Prompt dataset collection\n",
    "    *  Instruction tuning\n",
    "  * distilabel: Critique task\n",
    "    * Automatic evaluation\n",
    "    * Dataset curation for SFT (Supervised Fine Tuning)\n",
    "\n",
    "\n",
    " \n",
    "### Model Benchmarks\n",
    "\n",
    "* AI2 reasoning challenge - grade school science questions\n",
    "* HellaSwag - Common sense\n",
    "* MMLU - Massive Multitask Manguage Understanding measure how diverse LLM knowledge is\n",
    "* TruthfulQA - How truthful is a model\n",
    "* WinoGrande - commonsense reasoning\n",
    "* GSM8K - maths reasoning\n",
    "\n",
    "* \n",
    "\n",
    "## Hardware accellerators (Low cost)\n",
    "\n",
    "* AI accelerator hardware - [Hailo](https://www.cnx-software.com/2024/04/04/hailo-10-m-2-key-m-module-brings-generative-ai-to-the-edge-with-up-to-40-tops-of-performance/)\n",
    "* Coral TPU\n",
    "* NVidia Jetson\n",
    "\n",
    "\n",
    "\n",
    "## Run transformers in the browser\n",
    "\n",
    "[Transformers.js](https://huggingface.co/docs/transformers.js/en/index)\n",
    "\n",
    "Syntax 740 podcast\n",
    "* Run in browser or on node server\n",
    "* ONNX model format\n",
    "* Microsoft **ONNX runtime**\n",
    "* HuggingFace convert models to ONNX\n",
    "* Run in browser\n",
    "  * No server compute\n",
    "  * Privacy - no data sent to servers\n",
    "  * Rich JS tools to interact with browser\n",
    "  * Everyone has browser\n",
    "  * Soon web GPU support - speed\n",
    "* Run on node JS server --> Faster\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82648e2a-1fda-49b8-9df9-97438619a205",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "\n",
    "## Survey Papers - arxiv.org\n",
    "* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "* [Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196.pdf)\n",
    "\n",
    "\n",
    "## HuggingFace\n",
    "\n",
    "* Hugging Face [EPAM profile](https://huggingface.co/epam)\n",
    "* HuggingFace [course](https://www.youtube.com/watch?v=00GKzGyWFEs&list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o) - Technical\n",
    "\n",
    "\n",
    "\n",
    "## Learnings\n",
    "\n",
    "* DeepLearning.AI [short courses](https://www.deeplearning.ai/short-courses/)\n",
    "* \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
