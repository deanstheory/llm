{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701766ba-8181-4632-b7ff-d1008334a6a5",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# Closed vs Open Models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Open vs close model ELO](./data/arena_elo.jpg)\n",
    "\n",
    "\n",
    "\n",
    "## Closed/Proprietary LLMs\n",
    "\n",
    "\n",
    "https://www.linkedin.com/pulse/deep-dive-opensource-llms-vs-proprietor-dr-rabi-prasad-kutuc/\n",
    "\n",
    "OpenAI, \n",
    "* Accessibility\n",
    "* Customizability\n",
    "* Vendor Support\n",
    "* Licensing\n",
    "* Advanced Features and Performance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Open LLM\n",
    "\n",
    "* Accessibility\n",
    "* Customizability\n",
    "* ommunity Support\n",
    "* Licensing\n",
    "* \n",
    "\n",
    "Open source vs open weight\n",
    "\n",
    "* \n",
    "\n",
    "\n",
    "Hugging Face\n",
    "\n",
    "\n",
    "\n",
    "## Licensing\n",
    "\n",
    "https://www.linkedin.com/pulse/deep-dive-opensource-llms-vs-proprietor-dr-rabi-prasad-kutuc/\n",
    "\n",
    "Commercial use\n",
    "\n",
    "Research use\n",
    "\n",
    "\n",
    "## Survey papers\n",
    "\n",
    "\n",
    "* A Survey of Large Language Models [arxiv.org paper](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed6db5-9861-43a6-abeb-e6a69996a6dd",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# Open Models\n",
    "\n",
    "* Availability of high-quality open base models (LLaMA, Mistral \n",
    "* Open weights isn’t open source unless they provide full access to their training set and source code.\n",
    "  * Open weight vs open source\n",
    "  * Licensing\n",
    "* In most other tasks than generalist chat, open-source is ahead thanks to customized models.\n",
    "\n",
    "\n",
    "\n",
    "Fewer parameters\n",
    "* 3B, 13B, 30B, 70B\n",
    "\n",
    "\n",
    "# Open source models\n",
    "\n",
    "* [Olmo](https://allenai.org/olmo)\n",
    "* GPT-NeoX, Pythia, OLMo, and Amber all have publicly available training data and OSI-licensed training and evaluation code, model weights, and partially trained checkpoints\n",
    "\n",
    "\n",
    "\n",
    "# Open weights models\n",
    "\n",
    "\n",
    "## LLaMa\n",
    "\n",
    "LLaMA (Large Language Model Meta AI) are a family of LLM models release by [Meta](https://ai.meta.com/blog/llama-2/). Originally model release to researchers but got leaked. Model weights are released to the research community under a noncommercial license.\n",
    "* LLaMA - Feb '23\n",
    "* LLaMA 2 - Jul '23\n",
    "* LLaMA 3 - Jun/Jul '24????\n",
    "\n",
    "* LLaMA is available on HuggingFace [Meta profile](https://huggingface.co/meta-llama)\n",
    "\n",
    "A large number of researchers have extended LLaMA models by either instruction tuning or continual pretraining\n",
    "  *  instruction tuning LLaMA has become a major approach to developing customized or specialized models, due to the relatively low computational costs.\n",
    "\n",
    "![LLaMA](./data/llama.png)\n",
    "\n",
    "\n",
    "\n",
    "  *  Stanford [Alpaca-52K](https://github.com/tatsu-lab/stanford_alpaca) instruction-following data generated by the techniques in the [Self-Instruct](https://github.com/yizhongw/self-instruct)\n",
    "  *  On the self-instruct evaluation set, Alpaca shows many behaviors similar to OpenAI’s text-davinci-003, but is also surprisingly small and easy/cheap to reproduce.\n",
    "  *  Alpaca: [fine-tune](https://github.com/tatsu-lab/stanford_alpaca?tab=readme-ov-file#fine-tuning) LLaMMA models using standard Hugging Face training code. Alpaca is very cost-effective for training ($500)\n",
    "  * [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)\n",
    "    * The cost of training Vicuna-13B is around $300.\n",
    "    * Non-commercial use\n",
    "\n",
    "\n",
    "\n",
    "## Mistral\n",
    "\n",
    "* Open weights NOT open source\n",
    "  * Open weights isn’t open source unless they provide full access to their training set and source code. In all respect to the capabilities of Mistral’s models, it is an extreme stretch to call company that’s dropping torrents of weight binary, an OPEN SOURCE\n",
    "  * Don't expose how training, recipe, how to collect the data, mixture of experts\n",
    "  * Currently focused on developer experience first\n",
    "  * Not just APIs --> because you need AI integrator\n",
    "* Weights available on HuggingFace [Mistral AI profile](https://huggingface.co/mistralai)\n",
    "\n",
    "* \n",
    "\n",
    "## Grok (X)\n",
    "\n",
    "* Open weights\n",
    "* Available on HuggingFace [xAI profile](https://huggingface.co/xai-org)\n",
    "\n",
    "* \n",
    "\n",
    "\n",
    "## DBRX\n",
    "\n",
    "* DBRX is a general purpose LLM created by [Databricks](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)\n",
    "* Mixture of experts (MoE) architecture\n",
    "  * Routrer\n",
    "* DBRX is fine-grained, meaning it uses a larger number of smaller experts. DBRX has 16 experts and chooses 4, \n",
    "* Weights available on HuggingFace [Databrick profile](https://huggingface.co/databricks)\n",
    "\n",
    "\n",
    "-----------------------\n",
    "\n",
    "\n",
    "## Open Models/Weights w/ Agentic workflow better than GPT4.0????\n",
    "\n",
    "Agentic Reasoning Design Pattern\n",
    "- Reflection\n",
    "- Tools\n",
    "- Planning\n",
    "- Multi-agent collaboration\n",
    "\n",
    "- Andrew Ng [Sequoia Talk](https://www.youtube.com/watch?v=sal78ACtGTc)\n",
    "  - Agentic (using agent) Workflows\n",
    "  - Agentic workflow MUCH better than Zero-shot workflow (even for older models)\n",
    "    - GPT3.5 with agent workflow much better that GPT4.0 zero-shot\n",
    "   \n",
    " - Harrison Chase [Sequoia Langchain Agents](https://www.youtube.com/watch?v=pBBe1pk8hf4)\n",
    "  - Planning Step (upfront) vs Reflection Steps (at end)\n",
    "  - Flow engineering: AlphaCodium flow paper\n",
    "     - Offload planning to human \n",
    "  - UX of agent apps\n",
    "     - Human in the loop?\n",
    "     - Rewind and edit?\n",
    "     - Memory of agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a47937-590d-4850-b643-6aa554549776",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "\n",
    "[Analyse expenses with local LLM](https://www.youtube.com/watch?v=h_GTxRFYETY)\n",
    "\n",
    "\n",
    "  * [Ollama](https://ollama.com/) <--\n",
    "    * Simple and fast\n",
    "    * Large range of [models](https://ollama.com/library) available\n",
    "    * ```ollama pull mistral```\n",
    "    * ```ollama run mistral```\n",
    "    * [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)\n",
    "      * ```ollama create my-model-name -f my-model-file```\n",
    "      * ```ollma run my-model-name```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2373172-daa4-4f1c-8081-29db30a6c62b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Running LLMs locally\n",
    "\n",
    "\n",
    "## 6 ways to run local LLM\n",
    "\n",
    "* [Youtube](https://www.youtube.com/watch?v=7jMIsmwocpM)\n",
    "      \n",
    "Way\n",
    "* Privay\n",
    "* Compliance\n",
    "* Trust\n",
    "* Cost????\n",
    "\n",
    "Leaked Google document: [We Have No Moat, And Neither Does OpenAI](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)\n",
    "* Open models going thru' very quick iterations (every 1~2 wks)\n",
    "* \n",
    "\n",
    "\n",
    "\n",
    "Locally\n",
    "* Laptop\n",
    "* No need to use external APIs\n",
    "* Own cloud with GPU/\n",
    "* Run on phones\n",
    "\n",
    "* 6 ways:\n",
    "  * [Ollama](https://ollama.com/) <--\n",
    "    * Simple and fast\n",
    "    * Large range of [models](https://ollama.com/library) available\n",
    "    * ```ollama pull mistral```\n",
    "    * ```ollama run mistral```\n",
    "    * [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)\n",
    "      * ```ollama create my-model-name -f my-model-file```\n",
    "      * ```ollma run my-model-name```\n",
    "  * HuggingFace [Transformers](https://huggingface.co/docs/transformers/en/index)\n",
    "    * Need to know how to code (Python)\n",
    "    * ML knowledge\n",
    "  * [LangChain](https://python.langchain.com/docs/integrations/platforms/)\n",
    "    * Run local and remote models\n",
    "    * Slow (Python)\n",
    "  * [LLaMa.cpp](https://github.com/ggerganov/llama.cpp) <--\n",
    "    * Fast\n",
    "    * Csan run bigger models on smaller hardware\n",
    "    * Uses **GGUF** format. Modern and efficient\n",
    "    * Limitedmodel support\n",
    "  * [llamafile](https://github.com/Mozilla-Ocho/llamafile)\n",
    "    * Embed model in executionable file and run anywhere\n",
    "    * Builds on llama.cpp\n",
    "    * Spped optimisation for various hardware - https://justine.lol/matmul/\n",
    "\n",
    "  * [GPT4ALL](https://gpt4all.io/index.html) <--\n",
    "    * Can add own docs for context\n",
    "    * Limited models\n",
    "\n",
    "\n",
    "* Quantisation\n",
    "\n",
    "Reduce memory footprint for the model weoghts\n",
    "\n",
    "\n",
    "\n",
    "## Popular LLM Families\n",
    "\n",
    "* OpenAI GPT family\n",
    "* Meta LLaMA family\n",
    "* Google PaLM family\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39add882-f717-4369-80d8-c2350c765761",
   "metadata": {},
   "source": [
    "# Open Model Licensing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f8da9-2c2a-41c7-817c-ab296fc0af1c",
   "metadata": {},
   "source": [
    "# LLM Leaderboard\n",
    "\n",
    "* HuggingFace [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "* LMSYS Chatbot Arena [leaderboard](https://chat.lmsys.org/?leaderboard)\n",
    "\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a122f-7ed7-46b3-b33d-d1a45b4c464f",
   "metadata": {},
   "source": [
    "# Ollama & Private Data\n",
    "\n",
    "[Ollama](https://ollama.com/) download and install on Mac/Linux/Windows\n",
    "\n",
    "Available Ollama [models](https://ollama.com/library)\n",
    "* [Mistral](https://ollama.com/library/mistral)\n",
    "* Mixtral-8x7B is more powerful and can handle extended conversations - Supported????\n",
    "\n",
    "Ollama has support for multi-modal LLMs, such as bakllava and llava.\n",
    "\n",
    "\n",
    "Ollama & [Langchain](https://python.langchain.com/docs/integrations/llms/ollama)\n",
    "\n",
    "\n",
    "Basic commands:\n",
    "\n",
    "* Fetch available LLM model via  ```ollama pull <name-of-model>```\n",
    "* On Mac, the models will be download to ```~/.ollama/models```\n",
    "* To view all pulled models, use ```ollama list```\n",
    "* To chat directly with a model from the command line, use ```ollama run <name-of-model>```\n",
    "\n",
    "\n",
    "\n",
    "--------------------\n",
    "## References:\n",
    "\n",
    "* Building a Multi-Document Chatbot Using [Mistral 7B, ChromaDB, and Langchain](https://www.e2enetworks.com/blog/building-a-multi-document-chatbot-using-mistral-7b-chromadb-and-langchain)\n",
    "* [Ask Your Web Pages Using Mistral-7b & LangChain](https://medium.com/@zekaouinoureddine/ask-your-web-pages-using-mistral-7b-langchain-f976e1e151ca)\n",
    "* [Ollama Python Library Released! How to implement Ollama RAG?](https://www.youtube.com/watch?v=4HfSfFvLn9Q)\n",
    "  * Ollama RAG [code](https://mer.vin/2024/01/ollama-rag/)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec921a3-2683-4dcd-a1a7-8c325c6e954c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Why don't scientists trust atoms?\\n\\nBecause they make up everything!\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from langchain_community.llms import Ollama\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e71e1-b736-4e47-9c28-1a1893b7d9ba",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba1322-3b54-495e-b256-13dd40643992",
   "metadata": {},
   "source": [
    "-----\n",
    "## Environment set up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c679757-3f50-4a2e-8021-5aab1a7d206d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain \\\n",
    "    tiktoken \\\n",
    "    ollama \\\n",
    "    pypdf \\\n",
    "    chromadb \\\n",
    "    pinecone-client \\\n",
    "    ipywidgets \\\n",
    "    langflow \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18f5a5-15e0-4d88-8faa-99371aca6c11",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# Mistral Embedding\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e45c7b5-64a2-4d45-8cd5-e92e4b6602cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---------------------------\n",
    "# Other\n",
    "\n",
    "## Model Fine-tuning\n",
    "\n",
    "* LoRA (Low Rank)\n",
    "  * reduces the size of the update matrices by a factor of up to several thousand.\n",
    "  * This allows model fine-tuning at a fraction of the cost (~$100) and time (<1 day).\n",
    "  * Being able to personalize a language model in a few hours on consumer hardware is a big deal\n",
    "  * Fine-tuning is stackable\n",
    "  * Trianing giant models from scratch expensive and time consuming\n",
    "* Data quality scales better than data size\n",
    "* [ORPO](https://huggingface.co/blog/mlabonne/orpo-llama-3) finetuning <--\n",
    "  * Instruction tuning and preference alignment\n",
    " \n",
    "## Instruction tuning vs fine-tuning???\n",
    "\n",
    "\n",
    "## Training\n",
    "\n",
    "* Pre-training --> Fine-tuning --> Alignment\n",
    "* Fine-tuning (--> $100s cost???)\n",
    "  * An important reason to fine-tune LLMs is to align the responses to the expectations humans will have when providing instructions through prompts. This is the so-called instruction tuning\n",
    "  * AI Alignment is the process of steering AI systems towards human goals, preferences, and principles.\n",
    "    * Does each new application or idea really needs a whole new model?? or just fine-tune\n",
    "    * Align with human preference using DPO\n",
    "* Current open source fine-tuning the LLM in three ways:\n",
    "  * Reinforcement learning with human feedback (RLHF)\n",
    "  * Supervised Fine-tuning (SFT)\n",
    "  * Direct Preference Optimization (DPO). 3 main steps:\n",
    "    * Supervised fine-tuned model\n",
    "      * The process of taking a pre-trained model and further training it on a new, labeled dataset, with the goal of adapting the model's parameters to perform a specific task more effectively.\n",
    "    * The process of annotating data with preference labels\n",
    "    * DPO training\n",
    "  * https://cameronrwolfe.substack.com/p/understanding-and-using-supervised <---\n",
    "  * https://medium.com/@anchen.li/fine-tune-llama-2-with-sft-and-dpo-8b57cf3ec69\n",
    "* Transformer Reinforcement Learning [TRL](https://huggingface.co/docs/trl/index)\n",
    "\n",
    "\n",
    "\n",
    "![How to build a LLM](./data/build_llm.png)\n",
    "\n",
    "\n",
    "### Synthetic data\n",
    "\n",
    "*[Distilabel](https://github.com/argilla-io/distilabel)\n",
    "  * Framework for synthetic data and AI feedback for AI engineers\n",
    "  * AI generated AI datasets\n",
    "  * distilabel: Preference task\n",
    "    * Supervised fine tuning\n",
    "    * RLHF and DPO\n",
    "  * distilabel: Self-instruct task\n",
    "    *  Prompt dataset collection\n",
    "    *  Instruction tuning\n",
    "  * distilabel: Critique task\n",
    "    * Automatic evaluation\n",
    "    * Dataset curation for SFT (Supervised Fine Tuning)\n",
    "\n",
    "\n",
    "\n",
    "## Model Blending\n",
    "\n",
    "* [Mergekit](https://github.com/arcee-ai/mergekit) - Python tool\n",
    "* Merging methods\n",
    "  * Task arithmetic\n",
    "  * Slerp\n",
    "  * Ties/Dare\n",
    "  * Passthrough\n",
    "* Only merge models with same architecture\n",
    "  * Use different fine-tuned models of a specific MML family\n",
    " \n",
    "### Model Benchmarks\n",
    "\n",
    "* AI2 reasoning challenge - grade school science questions\n",
    "* HellaSwag - Common sense\n",
    "* MMLU - Massive Multitask Manguage Understanding measure how diverse LLM knowledge is\n",
    "* TruthfulQA - How truthful is a model\n",
    "* WinoGrande - commonsense reasoning\n",
    "* GSM8K - maths reasoning\n",
    "\n",
    "* \n",
    "\n",
    "## Hardware accelrators (Low cost)\n",
    "\n",
    "* AI accelerator hardware - [Hailo](https://www.cnx-software.com/2024/04/04/hailo-10-m-2-key-m-module-brings-generative-ai-to-the-edge-with-up-to-40-tops-of-performance/)\n",
    "* Coral TPU\n",
    "* NVidia Jetson\n",
    "\n",
    "\n",
    "* https://groq.com/\n",
    "\n",
    "\n",
    "\n",
    "## Run transformers in the browser\n",
    "\n",
    "[Transformers.js](https://huggingface.co/docs/transformers.js/en/index)\n",
    "\n",
    "Syntax 740 podcast\n",
    "* Run in browser or on node server\n",
    "* ONNX model format\n",
    "* Microsoft **ONNX runtime**\n",
    "* HuggingFace convert models to ONNX\n",
    "* Run in browser\n",
    "  * No server compute\n",
    "  * Privacy - no data sent to servers\n",
    "  * Rich JS tools to interact with browser\n",
    "  * Everyone has browser\n",
    "  * Soon web GPU support - speed\n",
    "* Run on node JS server --> Faster\n",
    "\n",
    "\n",
    "\n",
    "## Embedding\n",
    "\n",
    "Massive Text Embedding Benchmark [MTEB](https://huggingface.co/blog/mteb)\n",
    "* Huggingface MTEB [leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "\n",
    "## Knowledge Graph vs RAPTOR vs RAG\n",
    "\n",
    "* [Is Tree-based RAG Struggling? Not with Knowledge Graphs!](https://www.youtube.com/watch?v=g1TzbKDNr7M)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82648e2a-1fda-49b8-9df9-97438619a205",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "* HuggingFace [course](https://www.youtube.com/watch?v=00GKzGyWFEs&list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o)\n",
    "\n",
    "\n",
    "## Arxiv.org\n",
    "* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "* [Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196.pdf)\n",
    "\n",
    "\n",
    "## HuggingFace\n",
    "\n",
    "* -->>> [Open Source Models with Hugging Face](https://learn.deeplearning.ai/courses/open-source-models-hugging-face)\n",
    "* Hugging Face [EPAM profile](https://huggingface.co/epam)\n",
    "\n",
    "\n",
    "\n",
    "Leaked Google document: [We Have No Moat, And Neither Does OpenAI](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ded868-a42b-4ad1-8eb7-dc9c1d3e110c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
