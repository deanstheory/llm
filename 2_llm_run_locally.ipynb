{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2373172-daa4-4f1c-8081-29db30a6c62b",
   "metadata": {},
   "source": [
    "# Running LLMs locally\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Open/Closed LLM\n",
    "\n",
    "\n",
    "![Survey of models](./data/survey_of_llms.png)\n",
    "\n",
    "A Survey of Large Language Models [arxiv.org paper](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "\n",
    "\n",
    "\n",
    "## Popular LLM Families\n",
    "\n",
    "* OpenAI GPT family\n",
    "* Meta LLaMA family\n",
    "* Google PaLM family\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed6db5-9861-43a6-abeb-e6a69996a6dd",
   "metadata": {},
   "source": [
    "# Open Models\n",
    "\n",
    "\n",
    "In most other tasks than generalist chat, open-source is ahead thanks to customized models.\n",
    "\n",
    "![Open vs close model ELO](./data/arena_elo.jpg)\n",
    "\n",
    "\n",
    "\n",
    "Fewer parameters\n",
    "* 3B, 13B, 30B, 70B\n",
    "\n",
    "\n",
    "# Open source models\n",
    "\n",
    "* [Olmo](https://allenai.org/olmo)\n",
    "* GPT-NeoX, Pythia, OLMo, and Amber all have publicly available training data and OSI-licensed training and evaluation code, model weights, and partially trained checkpoints\n",
    "\n",
    "\n",
    "\n",
    "# Open weights models\n",
    "\n",
    "\n",
    "## Mistral\n",
    "\n",
    "* Open weights NOT open source\n",
    "  * Open weights isn’t open source unless they provide full access to their training set and source code. In all respect to the capabilities of Mistral’s models, it is an extreme stretch to call company that’s dropping torrents of weight binary, an OPEN SOURCE\n",
    "  * Don't expose how training, recipe, how to collect the data, mixture of experts\n",
    "  * Currently focused on developer experience first\n",
    "  * Not just APIs --> because you need AI integrator\n",
    "\n",
    "\n",
    "## Grok (X)\n",
    "\n",
    "* Open weights\n",
    "\n",
    "\n",
    "## LLaMa\n",
    "\n",
    "LLaMA (Large Language Model Meta AI) are a family of LLM models release by [Meta](https://ai.meta.com/blog/llama-2/). Model weights are released to the research community under a noncommercial license.\n",
    "* LLaMA - Feb '23\n",
    "* LLaMA 2 - Jul '23\n",
    "* LLaMA 3 - Jun/Jul '24????\n",
    "\n",
    "A large number of researchers have extended LLaMA models by either instruction tuning or continual pretraining\n",
    "  *  instruction tuning LLaMA has become a major approach to developing customized or specialized models, due to the relatively low computational costs.\n",
    "\n",
    "![LLaMA](./data/llama.png)\n",
    "\n",
    "\n",
    "\n",
    "  *  Stanford [Alpaca-52K](https://github.com/tatsu-lab/stanford_alpaca) instruction-following data generated by the techniques in the [Self-Instruct](https://github.com/yizhongw/self-instruct)\n",
    "  *  On the self-instruct evaluation set, Alpaca shows many behaviors similar to OpenAI’s text-davinci-003, but is also surprisingly small and easy/cheap to reproduce.\n",
    "  *  Alpaca: [fine-tune](https://github.com/tatsu-lab/stanford_alpaca?tab=readme-ov-file#fine-tuning) LLaMMA models using standard Hugging Face training code. Alpaca is very cost-effective for training ($500)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Open Models/Weights w/ Agentic workflow better than GPT4.0????\n",
    "\n",
    "Agentic Reasoning Design Pattern\n",
    "- Reflection\n",
    "- Tools\n",
    "- Planning\n",
    "- Multi-agent collaboration\n",
    "\n",
    "- Andrew Ng [Sequoia Talk](https://www.youtube.com/watch?v=sal78ACtGTc)\n",
    "  - Agentic (using agent) Workflows\n",
    "  - Agentic workflow MUCH better than Zero-shot workflow (even for older models)\n",
    "    - GPT3.5 with agent workflow much better that GPT4.0 zero-shot\n",
    "   \n",
    " - Harrison Chase [Sequoia Langchain Agents](https://www.youtube.com/watch?v=pBBe1pk8hf4)\n",
    "  - Planning Step (upfront) vs Reflection Steps (at end)\n",
    "  - Flow engineering: AlphaCodium flow paper\n",
    "     - Offload planning to human \n",
    "  - UX of agent apps\n",
    "     - Human in the loop?\n",
    "     - Rewind and edit?\n",
    "     - Memory of agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39add882-f717-4369-80d8-c2350c765761",
   "metadata": {},
   "source": [
    "# Open Model Licensing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a122f-7ed7-46b3-b33d-d1a45b4c464f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ollama & Private Data\n",
    "\n",
    "[Ollama](https://ollama.com/) download and install on Mac/Linux/Windows\n",
    "\n",
    "Available Ollama [models](https://ollama.com/library)\n",
    "* [Mistral](https://ollama.com/library/mistral)\n",
    "* Mixtral-8x7B is more powerful and can handle extended conversations - Supported????\n",
    "\n",
    "Ollama has support for multi-modal LLMs, such as bakllava and llava.\n",
    "\n",
    "\n",
    "Ollama & [Langchain](https://python.langchain.com/docs/integrations/llms/ollama)\n",
    "\n",
    "\n",
    "Basic commands:\n",
    "\n",
    "* Fetch available LLM model via  ```ollama pull <name-of-model>```\n",
    "* On Mac, the models will be download to ```~/.ollama/models```\n",
    "* To view all pulled models, use ```ollama list```\n",
    "* To chat directly with a model from the command line, use ```ollama run <name-of-model>```\n",
    "\n",
    "\n",
    "\n",
    "--------------------\n",
    "## References:\n",
    "\n",
    "* Building a Multi-Document Chatbot Using [Mistral 7B, ChromaDB, and Langchain](https://www.e2enetworks.com/blog/building-a-multi-document-chatbot-using-mistral-7b-chromadb-and-langchain)\n",
    "* [Ask Your Web Pages Using Mistral-7b & LangChain](https://medium.com/@zekaouinoureddine/ask-your-web-pages-using-mistral-7b-langchain-f976e1e151ca)\n",
    "* [Ollama Python Library Released! How to implement Ollama RAG?](https://www.youtube.com/watch?v=4HfSfFvLn9Q)\n",
    "  * Ollama RAG [code](https://mer.vin/2024/01/ollama-rag/)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec921a3-2683-4dcd-a1a7-8c325c6e954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.llms import Ollama\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e71e1-b736-4e47-9c28-1a1893b7d9ba",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba1322-3b54-495e-b256-13dd40643992",
   "metadata": {},
   "source": [
    "-----\n",
    "## Environment set up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c679757-3f50-4a2e-8021-5aab1a7d206d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain \\\n",
    "    tiktoken \\\n",
    "    ollama \\\n",
    "    pypdf \\\n",
    "    chromadb \\\n",
    "    pinecone-client \\\n",
    "    ipywidgets \\\n",
    "    langflow \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18f5a5-15e0-4d88-8faa-99371aca6c11",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# Mistral Embedding\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b5c52-1717-408b-ac65-feb90db1af3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---------\n",
    "# Data Ingestion\n",
    "\n",
    "\n",
    "<img src='./data/data_ingestion.png' width='800'>\n",
    "\n",
    "\n",
    "\n",
    "--> Point to data source and load multiple documents (PDF/Word/HTML/Chat...). [Document Loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)\n",
    "\n",
    "--> **Chunk** into smaller parts. [Text Splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)\n",
    "  * Optimize for the smallest size without losing context\n",
    "  * Consider adding some meaningful global metadata in all the chunks giving global context to all your embedded chunks\n",
    "  * Use ```chunk_overlap``` to maintain some local context\n",
    "  \n",
    "--> Create **embedding** vectors for each chunk using LLM embedding. [Text Embedding Models](https://python.langchain.com/en/latest/modules/models/text_embedding.html)\n",
    "  * An embedding is a vector (list) of floating point numbers\n",
    "  * Embeddings are an AI native way to represent any kind of data: **text, images, audio and video**\n",
    "        \n",
    "--> Store embedding + metadata in \n",
    "        [Vector stores](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)\n",
    "\n",
    "  * **Vector stores**:\n",
    "    * [Pinecone](https://docs.pinecone.io/docs/overview): Managed vector store. Pinecone vector search index (OpenAI dimension: 1536)\n",
    "    * [Chroma](https://docs.trychroma.com/): Open source locally managed vector store.\n",
    "    * [Qdrant](https://github.com/qdrant/qdrant): Open source vectorstore with local and cloud managed options\n",
    "    * PostgreSQL with [pg_vector](https://github.com/pgvector/pgvector)\n",
    "\n",
    "    \n",
    "--> **Semantic search** to retrieve relevant information by measuring the similarity between two vectors.\n",
    "  * Typical similarity metrics: **Cosine, Dot Product, Euclidean** \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ed6c0-2b29-4ff7-9194-50757e3a0dbb",
   "metadata": {},
   "source": [
    "-----\n",
    "## Load documents and chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6c0b6-cb96-4158-8db1-bb5816349913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting some variable used global for the following cells\n",
    "import os\n",
    "\n",
    "persist_chroma_directory = '.chroma_db'\n",
    "pdf_folder = './data/pdf'\n",
    "\n",
    "os.listdir(pdf_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a0f33-2b37-4923-b6f2-48cf94b13de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, \\\n",
    "                                        PyPDFLoader, \\\n",
    "                                        UnstructuredPDFLoader, \\\n",
    "                                        TextLoader\n",
    "\n",
    "loader = DirectoryLoader(pdf_folder, glob='**/*.pdf', loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# If using PyPDFLoader each document in documents is 1 page of a pdf. \n",
    "print(f'{len(documents)} pages loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87245c39-f766-4efc-b485-e831b5bb1b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f4a05-c6ce-4235-bc5c-0d08326831b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3edef-c598-4e60-baa3-3a47efcdfdf6",
   "metadata": {},
   "source": [
    "----\n",
    "## Split in to smaller chunks\n",
    "\n",
    "* Split the text up into small, semantically meaningful chunks.\n",
    "* Most LLMs are constrained by the number of tokens that you can pass in so passing in an entire document or several document pages + prompt may exceed LLM token limit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84b93e-33dd-4851-bfcf-84653f62be2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Chunk loaded documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f'{len(chunks)} chunks created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d49ae-a824-4d7b-99c1-e1d26c13d401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c68da4-40de-483a-a39b-d133c65ee3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ea33d-2064-486a-a140-7300e00710c8",
   "metadata": {},
   "source": [
    "----\n",
    "## Chroma: Create document embeddings\n",
    "\n",
    "[Chroma](https://docs.trychroma.com/): Open source locally managed vector store.\n",
    "\n",
    "Mistral [embedding pricing](https://docs.mistral.ai/platform/pricing/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d14dfe-d15b-44be-92fd-16450abda218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "persist_chroma_directory = '.chroma_db'\n",
    "\n",
    "\n",
    "#embedding = MistralAIEmbeddings(mistral_api_key=\"your-api-key\")\n",
    "#embedding.model = \"mistral-embed\"  # or your preferred model if available\n",
    "# use OpenAI embedding\n",
    "#embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, \\\n",
    "#                             model='text-embedding-ada-002')\n",
    "\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "chroma_store = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_chroma_directory)\n",
    "\n",
    "# Persist the database --> Need to call persist() when using Jupyter\n",
    "chroma_store.persist()\n",
    "chroma_store = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e8ff0-d67f-48df-ba71-3652439a4075",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------\n",
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "\n",
    "<img src='./data/RAG2.png' width='1000'>\n",
    "\n",
    "RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. \n",
    "\n",
    "The idea of [Retrieval Augmented Generation (RAG)](https://huggingface.co/docs/transformers/model_doc/rag) workflow is simple. Instead of asking a question directly, the process first uses the user question to perform a search to retrieve relevant documents from the internal dataset and then provides these documents together with the question to LLM. With the additional context the LLM can answer as though it has been trained with the internal dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c79dc4e5-5767-44c5-8aa8-552e1257b141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Compensation for Flight Cancellations by Aer Lingus (as per Article 3.AER LINGUS DELAY NOTICE):\n",
      "\n",
      "- Applicability:\n",
      "  * For flights departing from an EU airport or a third country to an EU airport with Aer Lingus as the operating carrier.\n",
      "  * Conditions: You have a confirmed reservation, present yourself for check-in on time, and are travelling at a publicly available fare.\n",
      "\n",
      "Rules for Assistance when a Flight is Cancelled:\n",
      "\n",
      "- When reasonably expected departure time exceeds:\n",
      "  * Two hours for flights of 1500 kilometres or less.\n",
      "  * Three hours for intra-Community flights over 1500 kilometres and other flights between 1500-3500 kilometres.\n",
      "  * Four hours for all other flights.\n",
      "- Free assistance:\n",
      "  * Meals and refreshments proportional to waiting time.\n",
      "  * Two telephone calls, telex or fax messages, or e-mails.\n",
      "- Additional assistance when departure time is at least a day later than originally announced:\n",
      "  * Hotel accommodation if a stay of one or more nights becomes necessary or additional to intended stay.\n",
      "  * Transport between the airport and place of accommodation (hotel or other).\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_chroma_directory = '.chroma_db'\n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "chroma_store = Chroma(embedding_function=embeddings, persist_directory=persist_chroma_directory)\n",
    "\n",
    "\n",
    "retriever = chroma_store.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the Ollama LLM function\n",
    "def ollama_llm(question, context):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response = ollama.chat(model='mistral', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "# Define the RAG chain\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    return ollama_llm(question, formatted_context)\n",
    "\n",
    "# Use the RAG chain\n",
    "#query = \"Provide details of compensation if my flight is cancelled? \"\n",
    "query = \"Provide details of compensation if my flight is cancelled? Output the results in bullet points\"\n",
    "#query = \"How much liquid can I bring on a flight?\"\n",
    "#query = \"how long is my ticket valid for?\"\n",
    "\n",
    "result = rag_chain(query)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69079c5b-3e02-4d5e-a682-8bc026644846",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "# Ollama RAG UI\n",
    "\n",
    "https://mer.vin/2024/01/ollama-rag/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e45c7b5-64a2-4d45-8cd5-e92e4b6602cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---------------------------\n",
    "# Other\n",
    "\n",
    "## Training\n",
    "\n",
    "* Pre-training --> Fine-tuning --> Alignment\n",
    "* Fine-tuning\n",
    "  * An important reason to fine-tune LLMs is to align the responses to the expectations humans will have when providing instructions through prompts. This is the so-called instruction tuning\n",
    "  * AI Alignment is the process of steering AI systems towards human goals, preferences, and principles.\n",
    "\n",
    "\n",
    "![How to build a LLM](./data/build_llm.png)\n",
    "\n",
    "\n",
    "\n",
    "## Model Blending\n",
    "\n",
    "* [Mergekit](https://github.com/arcee-ai/mergekit) - Python tool\n",
    "* Merging methods\n",
    "  * Task arithmetic\n",
    "  * Slerp\n",
    "  * Ties/Dare\n",
    "  * Passthrough\n",
    "* Only merge models with same architecture\n",
    "  * Use different fine-tuned models of a specific MML family\n",
    " \n",
    "### Model Benchmarks\n",
    "\n",
    "* AI2 reasoning challenge - grade school science questions\n",
    "* HellaSwag - Common sense\n",
    "* MMLU - Massive Multitask Manguage Understanding measure how diverse LLM knowledge is\n",
    "* TruthfulQA - How truthful is a model\n",
    "* WinoGrande - commonsense reasoning\n",
    "* GSM8K - maths reasoning\n",
    "\n",
    "* \n",
    "\n",
    "## Hardware accelrators (Low cost)\n",
    "\n",
    "* AI accelerator hardware - [Hailo](https://www.cnx-software.com/2024/04/04/hailo-10-m-2-key-m-module-brings-generative-ai-to-the-edge-with-up-to-40-tops-of-performance/)\n",
    "* Coral TPU\n",
    "* NVidia Jetson\n",
    "* \n",
    "\n",
    "\n",
    "\n",
    "## Run transformers in the browser\n",
    "\n",
    "[Transformers.js](https://huggingface.co/docs/transformers.js/en/index)\n",
    "\n",
    "Syntax 740 podcast\n",
    "* Run in browser or on node server\n",
    "* ONNX model format\n",
    "* Microsoft **ONNX runtime**\n",
    "* Huggingface convert models to ONNX\n",
    "* Run in browser\n",
    "  * No server compute\n",
    "  * Privacy - no data sent to servers\n",
    "  * Rich JS tools to interact with browser\n",
    "  * Everyone has browser\n",
    "  * Soon web GPU support - speed\n",
    "* Run on node JS server --> Faster\n",
    "\n",
    "\n",
    "\n",
    "## Embedding\n",
    "\n",
    "Massive Text Embedding Benchmark [MTEB](https://huggingface.co/blog/mteb)\n",
    "* Huggingface MTEB [leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82648e2a-1fda-49b8-9df9-97438619a205",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "* Huggingface [course](https://www.youtube.com/watch?v=00GKzGyWFEs&list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o)\n",
    "\n",
    "\n",
    "## Arxiv.org\n",
    "* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "* [Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196.pdf)\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ded868-a42b-4ad1-8eb7-dc9c1d3e110c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
