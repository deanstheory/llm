{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701766ba-8181-4632-b7ff-d1008334a6a5",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# Closed vs Open Models\n",
    "\n",
    "\n",
    "Proprietary model trained on vast amounts of data at considerable expense. Some companies have released smaller version of these models under different open frameworks and licenses\n",
    "\n",
    "\n",
    "## Closed/Proprietary LLMs\n",
    "\n",
    "OpenAI’s ChatGPT or Anthropic’s Claude are examples of proprietary systems where the public can’t access the code and model weights\n",
    "\n",
    "Closed source LLMs are language models where the model source or weights are not publicly available. Often developed by companies with significant resource for development and improvement. \n",
    "* Access may be restricted or a paid subsciption\n",
    "* Customisation may be limited as access to underlaying code and architecture mat be limit. Limiting customization to fine-tuning on pre-trained models and parmeter configuration\n",
    "* Vendor Support\n",
    "* Proprietary licensing\n",
    "* Advanced Features and Performance. Proprietary models usually maximum performance\n",
    "\n",
    "eg: OpenAI ChatGPT, Google Gemini, Antropic Claude, Cohere Command\n",
    "\n",
    "\n",
    "## Open LLM\n",
    "\n",
    "Most of the top performing open models are derived from closed models or much large models\n",
    "\n",
    "Popular LLM Families\n",
    "* OpenAI GPT family\n",
    "* Meta LLaMA family\n",
    "* Google PaLM family\n",
    "\n",
    "\n",
    "bfgb\n",
    "* Leaked LLama wieghts - Mar '23\n",
    "* Leaked Google document: [We Have No Moat, And Neither Does OpenAI](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)\n",
    " * Open models going thru' very quick iterations (every 1~2 wks)\n",
    "* Community is driving innovaiton\n",
    "  * Quantisation\n",
    "  * LORa\n",
    "\n",
    "\n",
    "\n",
    "Open models are freely available to the public. This fosters innovation, collaboration, and community driven development\n",
    "* Customizability\n",
    "* Support is via the community. This can be large community of developers depending on the model\n",
    "* Licensing needs to be reviewed if commercial use or research use\n",
    "* Transparent development which can help build trust\n",
    "* Availability of high-quality open base models (LLaMA, Mistral \n",
    "* Fewer parameters: 3B, 13B, 30B, 70B\n",
    "  \n",
    "eg. LLaMA family, Mistral, DBRX\n",
    "\n",
    "\n",
    "### Open source vs open weight\n",
    "\n",
    "The core question is whether simply releasing a model’s weights while keeping training methodology and data proprietary can be considered true open sourcing. \n",
    "\n",
    "\n",
    "\n",
    "Releasing only a model's weights broadly enables application development but concentrates control among a small group of organizations. Enabling open source access distributes control but requires greater commitment to transparency and decentralization.\n",
    "\n",
    "* Open source\n",
    "  * releasing a model as open source would entail providing the full source code and information required for retraining the model from scratch. This includes the model architecture code, training methodology and hyperparameters, the original training dataset, documentation, and other relevant details.\n",
    "  * open source enables model understanding and customization but requires substantially more work to release.\n",
    "  * [Olmo](https://allenai.org/olmo)\n",
    "\n",
    "\n",
    "* Open weights isn’t open source unless they provide full access to their training set and source code.\n",
    "  * Open weights refers to releasing only the pretrained parameters or weights of the neural network model itself.\n",
    "  * This allows others to use the model for inference and fine-tuning.\n",
    "  * However, the training code, original dataset, model architecture details, and training methodology is not provided.\n",
    "  * open weights allows model use but not full transparency\n",
    "\n",
    "\n",
    "\n",
    "![Open vs close model ELO](./data/arena_elo.jpg)\n",
    "\n",
    "\n",
    "\n",
    "-----------------\n",
    "References:\n",
    "\n",
    "* https://www.linkedin.com/pulse/deep-dive-opensource-llms-vs-proprietor-dr-rabi-prasad-kutuc/\n",
    "* https://promptengineering.org/llm-open-source-vs-open-weights-vs-restricted-weights/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c3452-b321-4a84-8943-20209e5844a5",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# Popular open models\n",
    "\n",
    "\n",
    "## LLaMa Family\n",
    "\n",
    "* LLaMA (Large Language Model Meta AI) are a family of LLM models release by [Meta](https://ai.meta.com/blog/meta-llama-3/). Originally model released to researchers under non-commercial license (Mar '23) however model weights were leaked.\n",
    "* A large number of researchers have extended LLaMA models by either instruction tuning or continual pretraining\n",
    "  *  instruction tuning LLaMA has become a major approach to developing customized or specialized models, due to the relatively low computational costs.\n",
    "* LLaMA is available on HuggingFace [Meta profile](https://huggingface.co/meta-llama)\n",
    "\n",
    "\n",
    "![LLaMA](./data/llama.png)\n",
    "\n",
    "\n",
    "\n",
    "*  Stanford [Alpaca-52K](https://github.com/tatsu-lab/stanford_alpaca) instruction-following data generated by the techniques in the [Self-Instruct](https://github.com/yizhongw/self-instruct)\n",
    "*  On the self-instruct evaluation set, Alpaca shows many behaviors similar to OpenAI’s text-davinci-003, but is also surprisingly small and easy/cheap to reproduce.\n",
    "*  Alpaca: [fine-tune](https://github.com/tatsu-lab/stanford_alpaca?tab=readme-ov-file#fine-tuning)\n",
    "*  LLaMMA models using standard Hugging Face training code\n",
    "*  Alpaca cost about \\$500 to train.\n",
    "* [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) fine-tuned LLaMa model using 70K user-shared ChatGPT conversations\n",
    "  * The cost of training Vicuna-13B is around \\$300.\n",
    "  * Non-commercial use\n",
    "\n",
    "\n",
    "\n",
    "---------------------\n",
    "## Mistral\n",
    "\n",
    "* Open weights NOT open source\n",
    "  * Open weights isn’t open source unless they provide full access to their training set and source code. In all respect to the capabilities of Mistral’s models, it is an extreme stretch to call company that’s dropping torrents of weight binary, an OPEN SOURCE\n",
    "  * Don't expose how training, recipe, how to collect the data, mixture of experts\n",
    "  * Currently focused on developer experience first\n",
    "  * Not just APIs --> because you need AI integrator\n",
    "* Weights available on HuggingFace [Mistral AI profile](https://huggingface.co/mistralai)\n",
    "\n",
    "\n",
    "\n",
    "----------------------\n",
    "## Grok (X)\n",
    "\n",
    "* Open weights\n",
    "* Available on HuggingFace [xAI profile](https://huggingface.co/xai-org)\n",
    "\n",
    "\n",
    "\n",
    "-----------------------\n",
    "## DBRX\n",
    "\n",
    "* DBRX is a general purpose LLM created by [Databricks](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)\n",
    "* Mixture of experts (MoE) architecture\n",
    "  * Routrer\n",
    "* DBRX is fine-grained, meaning it uses a larger number of smaller experts. DBRX has 16 experts and chooses 4, \n",
    "* Weights available on HuggingFace [Databrick profile](https://huggingface.co/databricks)\n",
    "\n",
    "\n",
    "-----------------------\n",
    "## References:\n",
    "\n",
    "*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a47937-590d-4850-b643-6aa554549776",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# Running LLMs locally\n",
    "\n",
    "\n",
    "* Laptop\n",
    "* No need to use external APIs\n",
    "* Own cloud with GPU/\n",
    "* * Groq\n",
    "* Run on phones\n",
    "\n",
    "  \n",
    "## Ollama\n",
    "\n",
    "[Analyse expenses with local LLM](https://www.youtube.com/watch?v=h_GTxRFYETY)\n",
    "\n",
    "\n",
    "  * [Ollama](https://ollama.com/) <--\n",
    "    * Simple and fast\n",
    "    * Large range of [models](https://ollama.com/library) available\n",
    "    * ```ollama pull mistral```\n",
    "    * ```ollama run mistral```\n",
    "    * [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)\n",
    "      * ```ollama create my-model-name -f my-model-file```\n",
    "      * ```ollma run my-model-name```\n",
    "     \n",
    "\n",
    "-----------------------\n",
    "## GTP4ALL\n",
    "\n",
    "  * [GPT4ALL](https://gpt4all.io/) <--\n",
    "    * Chat with you private data. Can add own docs for context\n",
    "    * Plug-in \n",
    "    * Limited models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-----------------------\n",
    "## Hugging Face\n",
    "\n",
    "  * HuggingFace [Transformers](https://huggingface.co/docs/transformers/en/index)\n",
    "    * Need to know how to code (Python)\n",
    "    * ML knowledge\n",
    "   \n",
    "\n",
    "\n",
    "-----------------------\n",
    "## LLaMa.cpp\n",
    "\n",
    "\n",
    "  * [LLaMa.cpp](https://github.com/ggerganov/llama.cpp) <--\n",
    "    * Fast\n",
    "    * Csan run bigger models on smaller hardware\n",
    "    * Uses **GGUF** format. Modern and efficient\n",
    "    * Limited model support\n",
    "   \n",
    "\n",
    "----------------------\n",
    "## Run transformers in the browser\n",
    "\n",
    "[Transformers.js](https://huggingface.co/docs/transformers.js/en/index)\n",
    "\n",
    "Syntax 740 podcast\n",
    "* Run in browser or on node server\n",
    "* ONNX model format\n",
    "* Microsoft **ONNX runtime**\n",
    "* HuggingFace convert models to ONNX\n",
    "* Run in browser\n",
    "  * No server compute\n",
    "  * Privacy - no data sent to servers\n",
    "  * Rich JS tools to interact with browser\n",
    "  * Everyone has browser\n",
    "  * Soon web GPU support - speed\n",
    "* Run on node JS server --> Faster\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2373172-daa4-4f1c-8081-29db30a6c62b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Running LLMs locally\n",
    "\n",
    "      \n",
    "Way??\n",
    "* Privacy\n",
    "* Compliance\n",
    "* Trust\n",
    "* Cost????\n",
    "* Smaller \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Locally\n",
    "  * [Ollama](https://ollama.com/) <--\n",
    "    * Simple and fast\n",
    "    * Large range of [models](https://ollama.com/library) available\n",
    "    * ```ollama pull mistral```\n",
    "    * ```ollama run mistral```\n",
    "    * [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)\n",
    "      * ```ollama create my-model-name -f my-model-file```\n",
    "      * ```ollma run my-model-name```\n",
    "  * [GPT4ALL](https://gpt4all.io/) <--\n",
    "    * Chat with you private data. Can add own docs for context\n",
    "    * Plug-in \n",
    "    * Limited models\n",
    "  * HuggingFace [Transformers](https://huggingface.co/docs/transformers/en/index)\n",
    "    * Need to know how to code (Python)\n",
    "    * ML knowledge\n",
    "  * [LLaMa.cpp](https://github.com/ggerganov/llama.cpp) <--\n",
    "    * Fast\n",
    "    * Csan run bigger models on smaller hardware\n",
    "    * Uses **GGUF** format. Modern and efficient\n",
    "    * Limited model support\n",
    "  \n",
    "  * [LangChain](https://python.langchain.com/docs/integrations/platforms/)\n",
    "    * Run local and remote models\n",
    "    * Slow (Python)\n",
    "\n",
    "  * [llamafile](https://github.com/Mozilla-Ocho/llamafile)\n",
    "    * Embed model in executionable file and run anywhere\n",
    "    * Builds on llama.cpp\n",
    "    * Spped optimisation for various hardware - https://justine.lol/matmul/\n",
    "\n",
    "\n",
    "\n",
    "### Quantisation\n",
    "\n",
    "Reduce memory footprint for the model weights\n",
    "\n",
    "\n",
    "### File formats\n",
    "\n",
    "* Model and fine-tuning file formats\n",
    "  * Uses **GGUF** format. Modern and efficient\n",
    "\n",
    "\n",
    "## Other alternatives - Groq\n",
    "\n",
    "* [Groq cloud](https://console.groq.com/playground)\n",
    "  * https://simonwillison.net/2024/Apr/22/llama-3/\n",
    "  * Serves at very high speeds - 800 tokens/sec\n",
    "  * Groq API (key required)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------------------------\n",
    "## References:\n",
    "\n",
    "* 6 Ways For Running A [Local LLM](https://semaphoreci.com/blog/local-llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f8da9-2c2a-41c7-817c-ab296fc0af1c",
   "metadata": {},
   "source": [
    "------------------------\n",
    "# LLM Leaderboard\n",
    "\n",
    "* LMSYS Chatbot Arena [leaderboard](https://chat.lmsys.org/?leaderboard)\n",
    "* [Benchmarking](https://chat.lmsys.org/) LLMs in the Wild\n",
    "* HuggingFace [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "\n",
    "\n",
    "\n",
    "## Model Benchmarks\n",
    "\n",
    "* AI2 reasoning challenge - grade school science questions\n",
    "* HellaSwag - Common sense\n",
    "* MMLU - Massive Multitask Manguage Understanding measure how diverse LLM knowledge is\n",
    "* TruthfulQA - How truthful is a model\n",
    "* WinoGrande - commonsense reasoning\n",
    "* GSM8K - maths reasoning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82648e2a-1fda-49b8-9df9-97438619a205",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# References\n",
    "\n",
    "\n",
    "## Survey Papers - arxiv.org\n",
    "* [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "* [Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196.pdf)\n",
    "\n",
    "\n",
    "## HuggingFace\n",
    "\n",
    "* Hugging Face [EPAM profile](https://huggingface.co/epam)\n",
    "* HuggingFace [course](https://www.youtube.com/watch?v=00GKzGyWFEs&list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o) - Technical\n",
    "\n",
    "\n",
    "\n",
    "## Learnings\n",
    "\n",
    "* DeepLearning.AI [short courses](https://www.deeplearning.ai/short-courses/)\n",
    "* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64925b52-c2f6-4e21-a988-36840abc8bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
