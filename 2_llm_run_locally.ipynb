{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0a122f-7ed7-46b3-b33d-d1a45b4c464f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ollama & Private Data\n",
    "\n",
    "[Ollama](https://ollama.com/) download and install on Mac/Linux/Windows\n",
    "\n",
    "Available Ollama [models](https://ollama.com/library)\n",
    "* [Mistral](https://ollama.com/library/mistral)\n",
    "* Mixtral-8x7B is more powerful and can handle extended conversations - Supported????\n",
    "\n",
    "Ollama has support for multi-modal LLMs, such as bakllava and llava.\n",
    "\n",
    "\n",
    "Ollama & [Langchain](https://python.langchain.com/docs/integrations/llms/ollama)\n",
    "\n",
    "\n",
    "Basic commands:\n",
    "\n",
    "* Fetch available LLM model via  ```ollama pull <name-of-model>```\n",
    "* On Mac, the models will be download to ```~/.ollama/models```\n",
    "* To view all pulled models, use ```ollama list```\n",
    "* To chat directly with a model from the command line, use ```ollama run <name-of-model>```\n",
    "\n",
    "\n",
    "\n",
    "--------------------\n",
    "## References:\n",
    "\n",
    "* Building a Multi-Document Chatbot Using [Mistral 7B, ChromaDB, and Langchain](https://www.e2enetworks.com/blog/building-a-multi-document-chatbot-using-mistral-7b-chromadb-and-langchain)\n",
    "* [Ask Your Web Pages Using Mistral-7b & LangChain](https://medium.com/@zekaouinoureddine/ask-your-web-pages-using-mistral-7b-langchain-f976e1e151ca)\n",
    "* [Ollama Python Library Released! How to implement Ollama RAG?](https://www.youtube.com/watch?v=4HfSfFvLn9Q)\n",
    "  * Ollama RAG [code](https://mer.vin/2024/01/ollama-rag/)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec921a3-2683-4dcd-a1a7-8c325c6e954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.llms import Ollama\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e71e1-b736-4e47-9c28-1a1893b7d9ba",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba1322-3b54-495e-b256-13dd40643992",
   "metadata": {},
   "source": [
    "-----\n",
    "## Environment set up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c679757-3f50-4a2e-8021-5aab1a7d206d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain \\\n",
    "    tiktoken \\\n",
    "    ollama \\\n",
    "    pypdf \\\n",
    "    chromadb \\\n",
    "    pinecone-client \\\n",
    "    ipywidgets \\\n",
    "    langflow \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18f5a5-15e0-4d88-8faa-99371aca6c11",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# Mistral Embedding\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b5c52-1717-408b-ac65-feb90db1af3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---------\n",
    "# Data Ingestion\n",
    "\n",
    "\n",
    "<img src='./data/data_ingestion.png' width='800'>\n",
    "\n",
    "\n",
    "\n",
    "--> Point to data source and load multiple documents (PDF/Word/HTML/Chat...). [Document Loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)\n",
    "\n",
    "--> **Chunk** into smaller parts. [Text Splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)\n",
    "  * Optimize for the smallest size without losing context\n",
    "  * Consider adding some meaningful global metadata in all the chunks giving global context to all your embedded chunks\n",
    "  * Use ```chunk_overlap``` to maintain some local context\n",
    "  \n",
    "--> Create **embedding** vectors for each chunk using LLM embedding. [Text Embedding Models](https://python.langchain.com/en/latest/modules/models/text_embedding.html)\n",
    "  * An embedding is a vector (list) of floating point numbers\n",
    "  * Embeddings are an AI native way to represent any kind of data: **text, images, audio and video**\n",
    "        \n",
    "--> Store embedding + metadata in \n",
    "        [Vector stores](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)\n",
    "\n",
    "  * **Vector stores**:\n",
    "    * [Pinecone](https://docs.pinecone.io/docs/overview): Managed vector store. Pinecone vector search index (OpenAI dimension: 1536)\n",
    "    * [Chroma](https://docs.trychroma.com/): Open source locally managed vector store.\n",
    "    * [Qdrant](https://github.com/qdrant/qdrant): Open source vectorstore with local and cloud managed options\n",
    "    * PostgreSQL with [pg_vector](https://github.com/pgvector/pgvector)\n",
    "\n",
    "    \n",
    "--> **Semantic search** to retrieve relevant information by measuring the similarity between two vectors.\n",
    "  * Typical similarity metrics: **Cosine, Dot Product, Euclidean** \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ed6c0-2b29-4ff7-9194-50757e3a0dbb",
   "metadata": {},
   "source": [
    "-----\n",
    "## Load documents and chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6c0b6-cb96-4158-8db1-bb5816349913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting some variable used global for the following cells\n",
    "import os\n",
    "\n",
    "persist_chroma_directory = '.chroma_db'\n",
    "pdf_folder = './data/pdf'\n",
    "\n",
    "os.listdir(pdf_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a0f33-2b37-4923-b6f2-48cf94b13de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, \\\n",
    "                                        PyPDFLoader, \\\n",
    "                                        UnstructuredPDFLoader, \\\n",
    "                                        TextLoader\n",
    "\n",
    "loader = DirectoryLoader(pdf_folder, glob='**/*.pdf', loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# If using PyPDFLoader each document in documents is 1 page of a pdf. \n",
    "print(f'{len(documents)} pages loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87245c39-f766-4efc-b485-e831b5bb1b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f4a05-c6ce-4235-bc5c-0d08326831b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3edef-c598-4e60-baa3-3a47efcdfdf6",
   "metadata": {},
   "source": [
    "----\n",
    "## Split in to smaller chunks\n",
    "\n",
    "* Split the text up into small, semantically meaningful chunks.\n",
    "* Most LLMs are constrained by the number of tokens that you can pass in so passing in an entire document or several document pages + prompt may exceed LLM token limit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84b93e-33dd-4851-bfcf-84653f62be2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Chunk loaded documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f'{len(chunks)} chunks created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d49ae-a824-4d7b-99c1-e1d26c13d401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c68da4-40de-483a-a39b-d133c65ee3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ea33d-2064-486a-a140-7300e00710c8",
   "metadata": {},
   "source": [
    "----\n",
    "## Chroma: Create document embeddings\n",
    "\n",
    "[Chroma](https://docs.trychroma.com/): Open source locally managed vector store.\n",
    "\n",
    "Mistral [embedding pricing](https://docs.mistral.ai/platform/pricing/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d14dfe-d15b-44be-92fd-16450abda218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "persist_chroma_directory = '.chroma_db'\n",
    "\n",
    "\n",
    "#embedding = MistralAIEmbeddings(mistral_api_key=\"your-api-key\")\n",
    "#embedding.model = \"mistral-embed\"  # or your preferred model if available\n",
    "# use OpenAI embedding\n",
    "#embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, \\\n",
    "#                             model='text-embedding-ada-002')\n",
    "\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "chroma_store = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_chroma_directory)\n",
    "\n",
    "# Persist the database --> Need to call persist() when using Jupyter\n",
    "chroma_store.persist()\n",
    "chroma_store = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e8ff0-d67f-48df-ba71-3652439a4075",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------\n",
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "\n",
    "<img src='./data/RAG2.png' width='1000'>\n",
    "\n",
    "RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. \n",
    "\n",
    "The idea of [Retrieval Augmented Generation (RAG)](https://huggingface.co/docs/transformers/model_doc/rag) workflow is simple. Instead of asking a question directly, the process first uses the user question to perform a search to retrieve relevant documents from the internal dataset and then provides these documents together with the question to LLM. With the additional context the LLM can answer as though it has been trained with the internal dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c79dc4e5-5767-44c5-8aa8-552e1257b141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Compensation for Flight Cancellations by Aer Lingus (as per Article 3.AER LINGUS DELAY NOTICE):\n",
      "\n",
      "- Applicability:\n",
      "  * For flights departing from an EU airport or a third country to an EU airport with Aer Lingus as the operating carrier.\n",
      "  * Conditions: You have a confirmed reservation, present yourself for check-in on time, and are travelling at a publicly available fare.\n",
      "\n",
      "Rules for Assistance when a Flight is Cancelled:\n",
      "\n",
      "- When reasonably expected departure time exceeds:\n",
      "  * Two hours for flights of 1500 kilometres or less.\n",
      "  * Three hours for intra-Community flights over 1500 kilometres and other flights between 1500-3500 kilometres.\n",
      "  * Four hours for all other flights.\n",
      "- Free assistance:\n",
      "  * Meals and refreshments proportional to waiting time.\n",
      "  * Two telephone calls, telex or fax messages, or e-mails.\n",
      "- Additional assistance when departure time is at least a day later than originally announced:\n",
      "  * Hotel accommodation if a stay of one or more nights becomes necessary or additional to intended stay.\n",
      "  * Transport between the airport and place of accommodation (hotel or other).\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_chroma_directory = '.chroma_db'\n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "chroma_store = Chroma(embedding_function=embeddings, persist_directory=persist_chroma_directory)\n",
    "\n",
    "\n",
    "retriever = chroma_store.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the Ollama LLM function\n",
    "def ollama_llm(question, context):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response = ollama.chat(model='mistral', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "# Define the RAG chain\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    return ollama_llm(question, formatted_context)\n",
    "\n",
    "# Use the RAG chain\n",
    "#query = \"Provide details of compensation if my flight is cancelled? \"\n",
    "query = \"Provide details of compensation if my flight is cancelled? Output the results in bullet points\"\n",
    "#query = \"How much liquid can I bring on a flight?\"\n",
    "#query = \"how long is my ticket valid for?\"\n",
    "\n",
    "result = rag_chain(query)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69079c5b-3e02-4d5e-a682-8bc026644846",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "# Ollama RAG UI\n",
    "\n",
    "https://mer.vin/2024/01/ollama-rag/\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
